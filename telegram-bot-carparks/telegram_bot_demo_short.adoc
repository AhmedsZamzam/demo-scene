= Building a Telegram bot against a REST endpoint with Go, Apache Kafka, and ksqlDB
Robin Moffatt <robin@confluent.io>
v0.01, 29 July 2020

== Making a Telegram bot

Check out the code at https://github.com/rmoff/golang-telegram-bot/blob/master/main.go

Run in 

[source,bash]
----
cd ~/git/golang-telegram-bot; go run .
----

Show it working by sending a message to https://t.me/rmoff_golang_bot[rmoff_golang_bot]

== Stream data from REST endpoint into Kafka

Stream the current data from the REST endpoint into Kafka, piping the output from `curl` into `kafkacat`, polling every three minutes: 

[source,bash]
----
while [ 1 -eq 1 ];
do 
    curl -s https://datahub.bradford.gov.uk/ebase/api/getData/v2/Council/CarParkCurrent | \
        tail -n +2 | \
        docker exec -i kafkacat kafkacat -b broker:29092 -t carparks -P -T
    sleep 180
done
----

== Apply a schema to the data with ksqlDB

[source,sql]
----
CREATE STREAM CARPARK_SRC (date          VARCHAR ,
                           time          VARCHAR ,
                           name          VARCHAR ,
                           capacity      INT ,
                           empty_places  INT ,
                           status        VARCHAR ,
                           latitude      DOUBLE ,
                           longitude     DOUBLE ,
                           directionsURL VARCHAR)
                WITH (KAFKA_TOPIC='carparks', 
                      VALUE_FORMAT='DELIMITED');
----

Use the schema to project columns: 

[source,sql]
----
SET 'auto.offset.reset' = 'earliest';
SELECT DATE, TIME, NAME, EMPTY_PLACES FROM CARPARK_SRC EMIT CHANGES LIMIT 5;
----

[source,sql]
----
+-------------+-------+-------------+-------------+
|DATE         |TIME   |NAME         |EMPTY_PLACES |
+-------------+-------+-------------+-------------+
|2020-07-28   |14:55  |Westgate     |73           |
|2020-07-28   |14:55  |Burnett St   |108          |
|2020-07-28   |14:55  |Crown Court  |94           |
|2020-07-28   |14:55  |NCP Hall Ings|505          |
----

== Transform the data

Create a new stream: 

* Add a source field for lineage
* Set the timestamp from the two source fields
* Make the location (lat/lon) a struct
* Serialise to Protobuf so that the schema is available for use downstream
** Could also use Avro or JSON Schema here

[source,sql]
----
SET 'auto.offset.reset' = 'earliest';

CREATE STREAM CARPARK_EVENTS WITH (VALUE_FORMAT='PROTOBUF') AS 
SELECT STRINGTOTIMESTAMP(DATE + ' ' + TIME ,'yyyy-MM-dd HH:mm','Europe/London' ) AS TS,
       *,
       STRUCT("lat" := LATITUDE, "lon":= LONGITUDE) AS "location",
       'v2/Council/CarParkCurrent' as SOURCE 
  FROM CARPARK_SRC 
  EMIT CHANGES;
----

Show that there are multiple results for a given car park: 

[source,sql]
----
SELECT TIMESTAMPTOSTRING( TS,'yyyy-MM-dd HH:mm:ss','Europe/London'), 
       NAME, 
       EMPTY_PLACES 
  FROM CARPARK_EVENTS 
  WHERE NAME='Westgate'
  EMIT CHANGES 
  LIMIT 3;
----

== Create a materialised view of the current state

[source,sql]
----
CREATE TABLE CARPARK AS
SELECT NAME, 
       TIMESTAMPTOSTRING(LATEST_BY_OFFSET(TS),
                         'yyyy-MM-dd HH:mm:ss','Europe/London') AS LATEST_TS, 
       LATEST_BY_OFFSET(CAPACITY)      AS CAPACITY,
       LATEST_BY_OFFSET(EMPTY_PLACES)  AS CURRENT_EMPTY_PLACES,
       (CAST(LATEST_BY_OFFSET(CAPACITY) - LATEST_BY_OFFSET(EMPTY_PLACES) AS DOUBLE) / 
        CAST(LATEST_BY_OFFSET(CAPACITY) AS DOUBLE)) * 100 AS PCT_FULL,
       LATEST_BY_OFFSET(STATUS)        AS STATUS,
       LATEST_BY_OFFSET(LATITUDE)      AS LATITUDE,
       LATEST_BY_OFFSET(LONGITUDE)     AS LONGITUDE,
       LATEST_BY_OFFSET(DIRECTIONSURL) AS DIRECTIONSURL
    FROM CARPARK_EVENTS
    GROUP BY NAME;
----

== K/V lookup: How many spaces are currently free?

[source,sql]
----
ksql> SELECT LATEST_TS, 
             CURRENT_EMPTY_PLACES
        FROM CARPARK 
       WHERE NAME='Westgate';
+----------------------+--------------------+
|CURRENT_EMPTY_PLACES  |PCT_FULL            |
+----------------------+--------------------+
|111                   |4.310344827586207   |
----

=== Use the REST API

[source,bash]
----
curl --silent --http2 --location --request POST 'http://localhost:8088/query-stream' \
--header 'Content-Type: application/vnd.ksql.v1+json; charset=utf-8' --header 'Accept: application/json' \
--data-raw '{"sql":"SELECT LATEST_TS, CURRENT_EMPTY_PLACES FROM CARPARK WHERE NAME='\''Westgate'\'';"}' | jq '.'
----

[source,javascript]
----
[
  {
    "queryId": null,
    "columnNames": [
      "LATEST_TS",
      "CURRENT_EMPTY_PLACES"
    ],
    "columnTypes": [
      "STRING",
      "INTEGER"
    ]
  },
  [
    "2020-07-29 15:01:00",
    73
  ]
]
----

== Event-driven alert: Tell me when there's a space available

[source,sql]
----
SELECT NAME AS CARPARK,
      TIMESTAMPTOSTRING(TS,'yyyy-MM-dd HH:mm:ss','Europe/London') AS DATA_TS,
      CAPACITY,
      EMPTY_PLACES
 FROM CARPARK_EVENTS 
 WHERE NAME = 'Kirkgate Centre' 
   AND EMPTY_PLACES > 0 
 EMIT CHANGES;
----

=== Use the REST API

[source,bash]
----
curl --http2 --location --request POST 'http://localhost:8088//query-stream' \
--header 'Content-Type: application/vnd.ksql.v1+json; charset=utf-8' \
--data-raw '{"properties":{"ksql.streams.auto.offset.reset": "latest"},
    "sql": "SELECT NAME AS CARPARK,      TIMESTAMPTOSTRING(TS,'\''yyyy-MM-dd HH:mm:ss'\'','\''Europe/London'\'') AS DATA_TS,      CAPACITY     ,      EMPTY_PLACES FROM CARPARK_EVENTS  WHERE NAME = '\''Kirkgate Centre'\''    AND EMPTY_PLACES > 0  EMIT CHANGES;"
}'
----

[source,bash]
----
{"queryId":"20a9c981-12d7-494e-a632-e6602b95ef96","columnNames":["CARPARK","DATA_TS","CAPACITY","EMPTY_PLACES"],"columnTypes":["STRING","STRING","INTEGER","INTEGER"]}
["Kirkgate Centre","2020-07-28 16:58:00",611,510]
----

== ksqlDB-powered Telegram bot

[source,bash]
----
cd ~/git/demo-scene/telegram-bot-carparks/go; go run .
----

== Analytics: Stream the data to Elasticsearch. 

Create a sink connector from ksqlDB: 

[source,sql]
----
CREATE SINK CONNECTOR SINK_ELASTIC_01 WITH (
  'connector.class'                     = 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector',
  'topics'                              = 'CARPARK_EVENTS',
  'key.converter'                       = 'org.apache.kafka.connect.storage.StringConverter',
  'value.converter'                     = 'io.confluent.connect.protobuf.ProtobufConverter',
  'value.converter.schema.registry.url' = 'http://schema-registry:8081',
  'connection.url'                      = 'http://elasticsearch:9200',
  'type.name'                           = '_doc',
  'key.ignore'                          = 'true',
  'schema.ignore'                       = 'true');
----

Check the status of the connector in ksqlDB

[source,sql]
----
SHOW CONNECTORS
----

[source,sql]
----
 Connector Name  | Type | Class                                                         | Status
----------------------------------------------------------------------------------------------------------------------
 SINK_ELASTIC_01 | SINK | io.confluent.connect.elasticsearch.ElasticsearchSinkConnector | RUNNING (1/1 tasks RUNNING)
----------------------------------------------------------------------------------------------------------------------
----

Check that data is arriving: 

[source,bash]
----
docker exec elasticsearch curl -s "http://localhost:9200/_cat/indices/*?h=idx,docsCount"
----

[source,bash]
----
.kibana_task_manager_1        2
.apm-agent-configuration      0
.kibana_1                     1
carpark_events           265793
----

Visualise it in Kibana: 

image::images/carpark_kibana01.png[]
