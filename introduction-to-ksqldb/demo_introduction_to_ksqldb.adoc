TODO: add instructions for cloning etc


Two terminal windows, both running ksqlDB CLI

[Window 1] Create a stream

[source,sql]
----
CREATE STREAM MOVEMENTS (LOCATION VARCHAR) 
    WITH (VALUE_FORMAT='JSON', PARTITIONS=1, KAFKA_TOPIC='movements');
----

[Window 2] Query the stream

[source,sql]
----
SELECT TIMESTAMPTOSTRING(ROWTIME,'yyyy-MM-dd HH:mm:ss','Europe/London') AS EVENT_TS, 
       ROWKEY AS PERSON, 
       LOCATION 
  FROM MOVEMENTS
  EMIT CHANGES;
----

[Window 1] Insert some data

[source,sql]
----
INSERT INTO MOVEMENTS VALUES ('robin', 'York');
INSERT INTO MOVEMENTS VALUES ('robin', 'Leeds');
INSERT INTO MOVEMENTS VALUES ('robin', 'Ilkley');
----

[Window 2] Query the stream with a predicate

[source,sql]
----
SET 'auto.offset.reset' = 'earliest';
SELECT TIMESTAMPTOSTRING(ROWTIME,'yyyy-MM-dd HH:mm:ss','Europe/London') AS EVENT_TS, 
       ROWKEY AS PERSON, 
       LOCATION 
  FROM MOVEMENTS
  WHERE LCASE(LOCATION)='leeds'
  EMIT CHANGES;
----

[Window 1] Insert some more data

[source,sql]
----
INSERT INTO MOVEMENTS VALUES ('robin', 'Sheffield');
INSERT INTO MOVEMENTS VALUES ('robin', 'Leeds');
INSERT INTO MOVEMENTS VALUES ('robin', 'Wakefield');
INSERT INTO MOVEMENTS VALUES ('robin', 'Leeds');
----

Show the topics

[source,sql]
----
SHOW TOPICS;
----

Create a new topic that could be used to trigger an event-driven app when the user is in a certain location

[source,sql]
----
CREATE STREAM LEEDS_USERS WITH (KAFKA_TOPIC='leeds-users') 
    AS SELECT * FROM MOVEMENTS WHERE LCASE(LOCATION)='leeds' EMIT CHANGES;
----

Show that a new topic has been created

[source,sql]
----
SHOW TOPICS;
----

Dump the topic contents

[source,sql]
----
PRINT 'leeds-users' FROM BEGINNING;
----

[Window 1] Insert some more data

[source,sql]
----
INSERT INTO MOVEMENTS VALUES ('robin', 'Sheffield');
INSERT INTO MOVEMENTS VALUES ('robin', 'Leeds');
----

-- Difference between stream and table
-- It's the same Kafka topic underneath

[source,sql]
----
CREATE TABLE MOVEMENTS_T (LOCATION VARCHAR) WITH (VALUE_FORMAT='JSON', KAFKA_TOPIC='movements');

SELECT ROWKEY AS PERSON, LOCATION FROM MOVEMENTS_T EMIT CHANGES;
SELECT ROWKEY AS PERSON, LOCATION FROM MOVEMENTS EMIT CHANGES;
----

[Window 2] Run aggregate query

[source,sql]
----
SELECT ROWKEY AS PERSON, 
       COUNT(*) AS LOCATION_CHANGES,
       COUNT_DISTINCT(LOCATION) AS UNIQUE_LOCATIONS 
  FROM MOVEMENTS 
GROUP BY ROWKEY 
EMIT CHANGES;
----

[Window 1] Insert some data
[source,sql]
----
INSERT INTO MOVEMENTS VALUES ('robin', 'Leeds');
INSERT INTO MOVEMENTS VALUES ('robin', 'London');
----









* TODO add in datagen
* TODO INSERT INTO to merge streams
-- TODO: Create sink connector to postgres

-- Include time in inserted data

INSERT INTO MOVEMENTS (ROWTIME, ROWKEY, LOCATION) VALUES (STRINGTOTIMESTAMP('2020-02-17T15:22:00Z','yyyy-MM-dd''T''HH:mm:ssX'), 'robin', 'Leeds');
INSERT INTO MOVEMENTS (ROWTIME, ROWKEY, LOCATION) VALUES (STRINGTOTIMESTAMP('2020-02-17T16:22:00Z','yyyy-MM-dd''T''HH:mm:ssX'), 'robin', 'Retford');
INSERT INTO MOVEMENTS (ROWTIME, ROWKEY, LOCATION) VALUES (STRINGTOTIMESTAMP('2020-02-17T17:23:00Z','yyyy-MM-dd''T''HH:mm:ssX'), 'robin', 'London');
INSERT INTO MOVEMENTS (ROWTIME, ROWKEY, LOCATION) VALUES (STRINGTOTIMESTAMP('2020-02-17T20:23:00Z','yyyy-MM-dd''T''HH:mm:ssX'), 'robin', 'The Parcel Yard');
INSERT INTO MOVEMENTS (ROWTIME, ROWKEY, LOCATION) VALUES (STRINGTOTIMESTAMP('2020-02-18T09:23:00Z','yyyy-MM-dd''T''HH:mm:ssX'), 'robin', 'Leeds');
INSERT INTO MOVEMENTS (ROWTIME, ROWKEY, LOCATION) VALUES (STRINGTOTIMESTAMP('2020-02-18T10:23:00Z','yyyy-MM-dd''T''HH:mm:ssX'), 'robin', 'Leeds');


SELECT TIMESTAMPTOSTRING(ROWTIME,'yyyy-MM-dd HH:mm:ss','Europe/London') AS EVENT_TS, ROWKEY AS Person, LOCATION AS Location FROM MOVEMENTS EMIT CHANGES;

+--------------------+-------+---------+
|EVENT_TS            |PERSON |LOCATION |
+--------------------+-------+---------+
|2020-02-17 15:22:00 |robin  |Leeds    |
|2020-02-17 17:23:00 |robin  |London   |
|2020-02-17 22:23:00 |robin  |Leeds    |
|2020-02-18 09:00:00 |robin  |Leeds    |


SET 'auto.offset.reset' = 'earliest';

CREATE TABLE PERSON_MOVEMENTS AS SELECT ROWKEY AS PERSON, COUNT_DISTINCT(LOCATION) AS UNIQUE_LOCATIONS, COUNT(*) AS LOCATION_CHANGES FROM MOVEMENTS GROUP BY ROWKEY;

SELECT UNIQUE_LOCATIONS, LOCATION_CHANGES FROM PERSON_MOVEMENTS WHERE PERSON='robin';

+-----------------+-----------------+
|UNIQUE_LOCATIONS |LOCATION_CHANGES |
+-----------------+-----------------+
| 1               | 


ksql> SELECT LOCATION_CHANGES, UNIQUE_LOCATIONS FROM PERSON_MOVEMENTS WHERE ROWKEY='robin' EMIT CHANGES;
+-----------------+-----------------+
|LOCATION_CHANGES |UNIQUE_LOCATIONS |
+-----------------+-----------------+
|1                |1                |
|2                |2                |
|3                |3                |
|4                |3                |

Press CTRL-C to interrupt




docker exec -t ksqldb curl -s -X "POST" "http://localhost:8088/ksql" \
     -H "Content-Type: application/vnd.ksql.v1+json; charset=utf-8" \
     -d '{"ksql":"CREATE STREAM MOVEMENTS (LOCATION VARCHAR) WITH (VALUE_FORMAT='\''JSON'\'', PARTITIONS=1, KAFKA_TOPIC='\''movements'\'');"}'


docker exec -t ksqldb curl -s -X "POST" "http://localhost:8088/ksql" \
     -H "Content-Type: application/vnd.ksql.v1+json; charset=utf-8" \
     -d '{
            "ksql":"CREATE STREAM LONDON AS SELECT * FROM MOVEMENTS WHERE LCASE(LOCATION)='\''london'\'';",
            "streamsProperties": {
                "ksql.streams.auto.offset.reset": "earliest"
            }
        }'

     