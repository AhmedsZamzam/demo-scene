# An introduction to KSQL 
Robin Moffatt <robin@confluent.io>
v1.00, July 2, 2019

## Setup

1. Clone the repository
+
[source,bash]
----
git clone https://github.com/confluentinc/demo-scene.git
----

2. Start up the environment
+
[source,bash]
----
cd ksql-intro
docker-compose up -d
----

3. Launch the KSQL CLI in *TWO* terminal windows (you'll need both)
+
[source,bash]
----
docker exec -it ksql-cli bash -c 'echo -e "\n\n⏳ Waiting for KSQL to be available before launching CLI\n"; while [ $(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/info) -ne 200 ] ; do echo -e $(date) "KSQL Server HTTP state: " $(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/info) " (waiting for 200)" ; sleep 1 ; done; ksql http://ksql-server:8088'
----


## Getting started

### Declare stream

[source,sql]
----
SHOW TOPICS;

PRINT 'orders';

CREATE STREAM ORDERS WITH 
  (VALUE_FORMAT='AVRO', 
   KAFKA_TOPIC ='orders');

SHOW STREAMS;

DESCRIBE ORDERS;

SELECT * FROM ORDERS LIMIT 10;
----

### Filter for given state

[source,sql]
----
SELECT * 
  FROM ORDERS 
 WHERE ADDRESS->STATE='New York';
----

### Persist to a new Kafka topic

[source,sql]
----
CREATE STREAM ORDERS_NY AS 
  SELECT * 
    FROM ORDERS 
   WHERE ADDRESS->STATE='New York';
----

Open a second terminal and run KSQL CLI 

[source,bash]
----
docker exec -it ksql-cli ksql http://ksql-server:8088
----

In the original window show the source messages: 

[source,sql]
----
PRINT 'orders';
----

Print the new topic in the second window:

[source,sql]
----
PRINT 'ORDERS_NY';
----

Optionally, use the terminal software's Find function to highlight all `New York` text in the source stream, and show that only these messages are written to the new topic.

### Drop field

[source,sql]
----
SELECT ORDERTIME, ORDERID, ITEMID, ORDERUNITS 
  FROM ORDERS;

CREATE STREAM ORDERS_NO_ADDRESS_DATA AS 
  SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') AS ORDER_TIMESTAMP, 
         ORDERID, 
         ITEMID, 
         ORDERUNITS 
    FROM ORDERS;
----

Observe that new schema has no address fields: 

[source,sql]
----
DESCRIBE EXTENDED ORDERS_NO_ADDRESS_DATA;
----

In second window, print the new topic:

[source,sql]
----
PRINT 'ORDERS_NO_ADDRESS_DATA';
----

In the original window show the source messages: 

[source,sql]
----
PRINT 'orders';
----

### Flatten schema & derive new columns

[source,sql]
----
CREATE STREAM ORDERS_FLAT AS 
  SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') AS ORDER_TIMESTAMP, 
         ORDERTIME AS ORDERTIME_EPOCH, 
         ORDERID, 
         ITEMID, 
         ORDERUNITS, 
         ADDRESS->STREET AS ADDRESS_STREET, 
         ADDRESS->CITY AS ADDRESS_CITY, 
         ADDRESS->STATE AS ADDRESS_STATE
    FROM ORDERS;
----

[source,sql]
----
PRINT 'ORDERS_FLAT';
----



### Reserialise to CSV

[source,sql]
----
CREATE STREAM ORDERS_FLAT_CSV WITH (VALUE_FORMAT='DELIMITED', 
                                    KAFKA_TOPIC='orders_csv') AS 
  SELECT * FROM ORDERS_FLAT;

PRINT 'orders_csv';
----

## Joins

### Join to items reference
[source,sql]
----
PRINT 'item_details_01' LIMIT 10;

CREATE TABLE ITEM_REFERENCE_01 WITH (VALUE_FORMAT='AVRO', 
                                     KAFKA_TOPIC='item_details_01', 
                                     KEY='ID');

DESCRIBE ITEM_REFERENCE_01;

SELECT TIMESTAMPTOSTRING(O.ROWTIME, 'yyyy-MM-dd HH:mm:ss') AS ORDER_TIMESTAMP, 
       O.ORDERID, 
       O.ITEMID, 
       I.MAKE, 
       I.COLOUR, 
       I.UNIT_COST, 
       O.ORDERUNITS, 
       O.ORDERUNITS * I.UNIT_COST AS TOTAL_ORDER_VALUE, 
       O.ADDRESS
  FROM ORDERS O 
       INNER JOIN ITEM_REFERENCE_01 I 
       ON O.ITEMID = I.ID 
 LIMIT 5;

CREATE STREAM ORDERS_ENRICHED AS 
SELECT O.ROWTIME AS ORDER_TIMESTAMP, 
       O.ORDERID, 
       O.ITEMID, 
       I.MAKE, 
       I.COLOUR, 
       I.UNIT_COST, 
       O.ORDERUNITS, 
       O.ORDERUNITS * I.UNIT_COST AS TOTAL_ORDER_VALUE, 
       O.ADDRESS
  FROM ORDERS O 
       INNER JOIN ITEM_REFERENCE_01 I 
       ON O.ITEMID = I.ID ;
----

Land to Elasticsearch

[source,bash]
----
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/sink-elastic-orders-01/config \
    -d '{
        "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
        "type.name": "type.name=kafkaconnect",
        "key.converter":"org.apache.kafka.connect.storage.StringConverter",
        "topics": "ORDERS_ENRICHED",
        "schema.ignore": "true",
        "connection.url": "http://elasticsearch:9200"
    }'
----

Check that the connector is `RUNNING`

[source,bash]
----
curl -s "http://localhost:8083/connectors"| \
  jq '.[]'| \
  xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| \
  jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| \
  column -s : -t| sed 's/\"//g'| sort
----

[source,bash]
----
sink-elastic-orders-01          |  RUNNING  |  RUNNING
source-datagen-item_details_01  |  RUNNING  |  FAILED
source-datagen-orders-uk        |  RUNNING  |  RUNNING
source-datagen-orders-us        |  RUNNING  |  RUNNING
----


View in http://localhost:5601/app/kibana#/management/kibana/index?_g=()[Kibana]

## Aggregates 

### Orders count by manufacturer

[source,sql]
----
SELECT MAKE, COUNT(*) AS ORDER_COUNT
  FROM ORDERS_ENRICHED 
  GROUP BY MAKE 
  LIMIT 5;
----

### Total order value per hour, by manufacturer

[source,sql]
----
SELECT TIMESTAMPTOSTRING(WINDOWSTART(),'yyyy-MM-dd HH:mm:ss') AS WINDOW_START_TS, 
       MAKE, 
       COUNT(*) AS ORDER_COUNT, 
       SUM(TOTAL_ORDER_VALUE) AS TOTAL_ORDER_VALUE 
  FROM ORDERS_ENRICHED 
         WINDOW TUMBLING (SIZE 1 HOUR) 
GROUP BY MAKE;
----

### Manufacturers for which there have been more than $10,000 of orders in an hour

[source,sql]
----
SELECT TIMESTAMPTOSTRING(WINDOWSTART(),'yyyy-MM-dd HH:mm:ss') AS WINDOW_START_TS, 
       MAKE, 
       COUNT(*) AS ORDER_COUNT, 
       SUM(TOTAL_ORDER_VALUE) AS TOTAL_ORDER_VALUE 
  FROM ORDERS_ENRICHED 
         WINDOW TUMBLING (SIZE 1 HOUR) 
GROUP BY MAKE 
HAVING SUM(TOTAL_ORDER_VALUE) > 10000;
----

## Merging and Splitting Streams

### Merging streams with INSERT INTO

Imagine you have two inbound streams of orders, from separate geographies (e.g. UK and US). You want to combine these into a single stream for use by consumers. 

Add the second stream, containing UK orders: 

[source,sql]
----
SHOW TOPICS;

CREATE STREAM ORDERS_UK WITH (VALUE_FORMAT='AVRO', KAFKA_TOPIC='orders_uk');

SELECT * FROM ORDERS_UK LIMIT 5;
----

Create the new combined stream, populated first by all US orders (the original `ORDERS` stream): 

[source,sql]
----
CREATE STREAM ORDERS_COMBINED AS 
  SELECT 'US' AS SOURCE, 
         'US-'+CAST(ORDERID AS VARCHAR) AS ORDERID, 
         ORDERTIME, 
         ITEMID, 
         ORDERUNITS, 
         ADDRESS 
    FROM ORDERS
    PARTITION BY ORDERID;
----

Add the source of UK order data: 

[source,sql]
----
INSERT INTO ORDERS_COMBINED 
  SELECT 'UK' AS SOURCE, 
         'UK-'+CAST(ORDERID AS VARCHAR) AS ORDERID, 
         ORDERTIME, 
         ITEMID, 
         ORDERUNITS, 
         ADDRESS 
    FROM ORDERS_UK
    PARTITION BY ORDERID;
----

[source,sql]
----
SET 'auto.offset.reset' = 'latest';

SELECT * FROM ORDERS_COMBINED LIMIT 20;
----

N.B. in KSQL 5.3 the above should be changed to a CREATE STREAM and two INSERT INTO, instead of a CSAS. 

### Splitting streams

Imagine you have only the single source of `ORDERS_COMBINED` and you want two separate streams of US and UK order data : 

[source,sql]
----
CREATE STREAM ORDER_SPLIT_US AS 
  SELECT * 
    FROM ORDERS_COMBINED 
   WHERE SOURCE ='US';

CREATE STREAM ORDER_SPLIT_UK AS 
  SELECT * 
    FROM ORDERS_COMBINED 
   WHERE SOURCE ='UK';

CREATE STREAM ORDER_SPLIT_OTHER AS 
  SELECT * 
    FROM ORDERS_COMBINED 
   WHERE SOURCE !='US' 
     AND SOURCE !='UK';
----

## Time handling

### Event time vs ingest time (`ORDERTIME` vs `ROWTIME`)

[source,sql]
----
SELECT TIMESTAMPTOSTRING(ORDERTIME,'yyyy-MM-dd HH:mm:ss'), 
       'Order ID : ' + CAST(ORDERID AS VARCHAR) AS ORDERID
  FROM ORDERS 
 WHERE ITEMID='Item_42' 
 LIMIT 5;
----

[source,sql]
----
2019-06-09 15:18:07 | Order ID : 25
2019-06-09 11:15:30 | Order ID : 224
2019-06-09 22:03:59 | Order ID : 246
2019-06-09 02:42:02 | Order ID : 257
2019-06-09 23:01:00 | Order ID : 362
----

[source,sql]
----
SELECT TIMESTAMPTOSTRING(WINDOWSTART(),'yyyy-MM-dd HH:mm:ss') AS WINDOW_START_TS, 
       ITEMID, 
       COUNT(*) AS ORDER_COUNT 
  FROM ORDERS 
         WINDOW TUMBLING (SIZE 1 HOUR) 
 WHERE ITEMID='Item_42' 
GROUP BY ITEMID;
----

[source,sql]
----
2019-06-11 10:00:00 | Item_42 | 1
2019-06-11 10:00:00 | Item_42 | 18
2019-06-11 10:00:00 | Item_42 | 19
----

[source,sql]
----
SELECT TIMESTAMPTOSTRING(ROWTIME,'yyyy-MM-dd HH:mm:ss'),
       TIMESTAMPTOSTRING(ORDERTIME,'yyyy-MM-dd HH:mm:ss'), 
       'Order ID : ' + CAST(ORDERID AS VARCHAR) AS ORDERID
  FROM ORDERS 
 WHERE ITEMID='Item_42' 
 LIMIT 5;
----

[source,sql]
----
2019-07-01 11:16:29 | 2019-06-09 15:18:07 | Order ID : 25
2019-07-01 11:17:20 | 2019-06-09 11:15:30 | Order ID : 224
2019-07-01 11:17:25 | 2019-06-09 22:03:59 | Order ID : 246
2019-07-01 11:17:28 | 2019-06-09 02:42:02 | Order ID : 257
2019-07-01 11:17:56 | 2019-06-09 23:01:00 | Order ID : 362
----


[source,sql]
----
CREATE STREAM ORDERS_BY_EVENTTIME WITH (VALUE_FORMAT='AVRO', 
                                        KAFKA_TOPIC='orders', 
                                        TIMESTAMP='ORDERTIME');
----

[source,sql]
----
SELECT TIMESTAMPTOSTRING(ROWTIME,'yyyy-MM-dd HH:mm:ss'),
       TIMESTAMPTOSTRING(ORDERTIME,'yyyy-MM-dd HH:mm:ss'), 
       'Order ID : ' + CAST(ORDERID AS VARCHAR) AS ORDERID
  FROM ORDERS_BY_EVENTTIME 
 WHERE ITEMID='Item_42'
 LIMIT 5;
----

[source,sql]
----
2019-06-09 11:40:16 | 2019-06-09 11:40:16 | 15
2019-06-09 12:49:45 | 2019-06-09 12:49:45 | 129
2019-06-09 19:50:33 | 2019-06-09 19:50:33 | 246
2019-06-09 23:02:23 | 2019-06-09 23:02:23 | 657
2019-06-09 05:22:04 | 2019-06-09 05:22:04 | 763
----

[source,sql]
----
SELECT TIMESTAMPTOSTRING(WINDOWSTART(),'yyyy-MM-dd HH:mm:ss') AS WINDOW_START_TS, 
       ITEMID, 
       COUNT(*) AS ORDER_COUNT 
  FROM ORDERS_BY_EVENTTIME 
         WINDOW TUMBLING (SIZE 1 HOUR) 
 WHERE ITEMID='Item_42' 
GROUP BY ITEMID;
----

[source,sql]
----
2019-06-09 08:00:00 | Item_42 | 1
2019-06-09 19:00:00 | Item_42 | 1
2019-06-09 23:00:00 | Item_42 | 2
----

## Analytics - pivoting & bucketing metrics

* Using `CASE` to bucket metrics:
+
[source,sql]
----
SELECT ORDERID, 
       ORDERUNITS,
       CASE WHEN ORDERUNITS > 15 THEN 'Really big order' 
            WHEN ORDERUNITS > 10 THEN 'Big order' 
                                 ELSE 'Normal order' 
         END AS ORDER_TYPE 
  FROM ORDERS_ENRICHED
  LIMIT 10;
----
+
[source,sql]
----
0 | 18 | Really big order
1 | 13 | Big order
3 | 5 | Normal order
0 | 8 | Normal order
1 | 5 | Normal order
2 | 5 | Normal order
3 | 15 | Big order
4 | 19 | Really big order
5 | 2 | Normal order
6 | 4 | Normal order
----

* Using `CASE` to create bucket metric aggregates
+
[source,sql]
----
SELECT CASE WHEN ORDERUNITS > 15 THEN 'Really big order' 
            WHEN ORDERUNITS > 10 THEN 'Big order' 
                                 ELSE 'Normal order' 
         END AS ORDER_TYPE,
       COUNT(*) 
  FROM ORDERS_ENRICHED 
GROUP BY CASE WHEN ORDERUNITS > 15 THEN 'Really big order' 
              WHEN ORDERUNITS > 10 THEN 'Big order' 
                                   ELSE 'Normal order' 
           END;
----

* Using `CASE` to pivot bucket aggregates
+
[source,sql]
----
SELECT TIMESTAMPTOSTRING(WINDOWSTART(),'yyyy-MM-dd HH:mm:ss') AS WINDOW_START_TS, 
       MAKE, 
       COUNT(*), 
       SUM(CASE WHEN ORDERUNITS > 15 THEN 1 ELSE 0 END) AS CT_REALLY_BIG_ORDER,
       SUM(CASE WHEN ORDERUNITS > 10 AND ORDERUNITS<15 THEN 1 ELSE 0 END) AS CT_BIG_ORDER,
       SUM(CASE WHEN ORDERUNITS <= 10 THEN 1 ELSE 0 END) AS CT_NORMAL_ORDER
  FROM ORDERS_ENRICHED 
        WINDOW TUMBLING (SIZE 1 HOUR)
GROUP BY MAKE
LIMIT 5;
----
+
[source,sql]
----
2019-07-01 11:00:00 | Hilpert and Sons | 1 | 1 | 0 | 0
2019-07-01 11:00:00 | Considine and Sons | 71 | 12 | 11 | 43
2019-07-01 11:00:00 | MacGyver Group | 63 | 14 | 11 | 33
2019-07-01 11:00:00 | Bauch-Hudson | 64 | 14 | 16 | 30
2019-07-01 11:00:00 | Corkery-Rath | 59 | 14 | 12 | 31
----

