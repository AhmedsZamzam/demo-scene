= From Zero to Hero with Kafka Connect - demo script
Robin Moffatt <robin@confluent.io>
v0.02, April 24, 2019

== Running the test rig

1. Bring up the stack
+
[source,bash]
----
git clone https://github.com/confluentinc/demo-scene.git
cd kafka-connect-zero-to-hero
docker-compose up -d
----
+
This brings up the stack ready for use. 

2. Wait for Kafka Connect to be started
+
[source,bash]
----
bash -c ' \
echo "Waiting for Kafka Connect to start listening on localhost ‚è≥"
while [ $(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors) -eq 000 ] ; do 
  echo -e $(date) " Kafka Connect listener HTTP state: " $(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors) " (waiting for 200)"
  sleep 5 
done
echo -e $(date) " Kafka Connect is ready! Listener HTTP state: " $(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors)
'
----


== Demo 1 - Kafka Connect pipelines

A MySQL database stores orders placed by an application. 

* Kafka Connect - Debezium MySQL connector streams them into Kafka topic
* Kafka Connect - Elasticsearch connector to stream orders from Kafka topic to Elasticsearch

=== Ingest data from a database

[source,bash]
----
curl -i -X POST -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/ \
    -d '{
      "name": "source-debezium-orders-00",
      "config": {
            "connector.class": "io.debezium.connector.mysql.MySqlConnector",
            "database.hostname": "mysql",
            "database.port": "3306",
            "database.user": "debezium",
            "database.password": "dbz",
            "database.server.id": "42",
            "database.server.name": "asgard",
            "table.whitelist": "demo.orders",
            "database.history.kafka.bootstrap.servers": "kafka:29092",
            "database.history.kafka.topic": "dbhistory.demo" ,
            "decimal.handling.mode": "double",
            "include.schema.changes": "true",
            "transforms": "unwrap,addTopicPrefix",
            "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
            "transforms.addTopicPrefix.type":"org.apache.kafka.connect.transforms.RegexRouter",
            "transforms.addTopicPrefix.regex":"(.*)",
            "transforms.addTopicPrefix.replacement":"mysql-debezium-$1"
       }
    }'
----

Trigger the MySQL data generator with: 

[source,bash]
----
docker-compose exec mysql /data/02_populate_more_orders.sh
----

Check the status of the connector

[source,bash]
----
curl -s "http://localhost:8083/connectors"| jq '.[]'| xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| column -s : -t| sed 's/\"//g'| sort
----

Output should be

[source,bash]
----
source-debezium-orders-00  |  RUNNING  |  RUNNING
----

http://localhost:9021/management/topics[View the topic in Confluent Control Center] or from the CLI: 

[source,bash]
----
docker-compose exec -T kafka-connect \
            kafka-avro-console-consumer \
            --bootstrap-server kafka:29092 \
            --property schema.registry.url=http://schema-registry:8081 \
            --topic mysql-debezium-asgard.demo.ORDERS | jq '.'
----

=== Stream data from Kafka to Elasticsearch

[source,bash]
----
curl -i -X POST -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/ \
    -d '{
      "name": "sink-elastic-orders-00",
      "config": {
        "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
        "topics": "mysql-debezium-asgard.demo.ORDERS",
        "key.ignore": "false",
        "schema.ignore": "true",
        "type.name": "type.name=kafkaconnect",
        "connection.url": "http://elasticsearch:9200",
        "transforms": "dropTopicPrefix,extractKey",
        "transforms.extractKey.type":"org.apache.kafka.connect.transforms.ExtractField$Key",
        "transforms.extractKey.field":"id",
        "transforms.dropTopicPrefix.type":"org.apache.kafka.connect.transforms.RegexRouter",
        "transforms.dropTopicPrefix.regex":"mysql-debezium-(.*)",
        "transforms.dropTopicPrefix.replacement":"$1"
      }
    }'
----


http://localhost:5601/app/kibana#/discover?_g=(refreshInterval:(pause:!f,value:5000),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:'asgard.demo.orders',interval:auto,query:(language:lucene,query:''),sort:!(CREATE_TS,desc))[Inspect the data in Kibana] or from CLI: 

[source,bash]
----
echo '{ "size": 1, "sort": [ { "CREATE_TS": { "order": "desc" } } ] }' |\
  http http://localhost:9200/asgard.demo.orders/_search
----

or

[source,bash]
----
curl -s http://localhost:9200/asgard.demo.orders/_search \
  -H 'content-type: application/json' \
  -d '{ "size": 1, "sort": [ { "CREATE_TS": { "order": "desc" } } ] }' |\
  jq '.'
----

== Demo 2 - Converters and Single Message Transform

Converters change how we serialise the data onto a topic (source), or deserialise data from a topic (sink). 

=== Converters 

Connect workers 

=== Single Message Transform with Elasticsearch sink

https://docs.confluent.io/current/connect/transforms/index.html[Single Message Transforms] can be used to apply transformations including: 

* Change the topic name (n.b. often used by sinks to define the target object name)
* Dropping fields
* Renaming fields
* Renaming the topic

Here the example is on a sink connector but SMT are equally applicable to source connectors too. 

* Remove the key from its struct
+
[source,bash]
----
{"id":41739}
----
+
becomes
+
[source,bash]
----
41739
----

* Remove part of the topic name
+
[source,bash]
----
mysql-debezium-asgard.demo.ORDERS
----
+
becomes
+
[source,bash]
----
asgard.demo.ORDERS
----

* Append a timestamp to the topic name (useful for time-based indices in Elasticsearch etc)
+
[source,bash]
----
asgard.demo.ORDERS
----
+
becomes
+
[source,bash]
----
asgard.demo.ORDERS-201905
----

* Rename a field
+
[source,bash]
----
delivery_address
----
+
becomes
+
[source,bash]
----
shipping_address
----

* Drop a field
+
[source,bash]
----
CREATE_TS
----
+
both get omitted from the target data

[source,bash]
----
curl -i -X POST -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/ \
    -d '{
      "name": "sink-elastic-orders-01",
      "config": {
        "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
        "topics": "mysql-debezium-asgard.demo.ORDERS",
        "key.ignore": "false",
        "schema.ignore": "true",
        "type.name": "type.name=kafkaconnect",
        "connection.url": "http://elasticsearch:9200",
        "transforms": "dropTopicPrefix,extractKey,addDateToTopic,renameField,dropField",
        "transforms.extractKey.type":"org.apache.kafka.connect.transforms.ExtractField$Key",
        "transforms.extractKey.field":"id",
        "transforms.dropTopicPrefix.type":"org.apache.kafka.connect.transforms.RegexRouter",
        "transforms.dropTopicPrefix.regex":"mysql-debezium-(.*)",
        "transforms.dropTopicPrefix.replacement":"$1",
        "transforms.addDateToTopic.type": "org.apache.kafka.connect.transforms.TimestampRouter",
        "transforms.addDateToTopic.topic.format": "${topic}-${timestamp}",
        "transforms.addDateToTopic.timestamp.format": "YYYYMM",
        "transforms.renameField.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
        "transforms.renameField.renames": "delivery_address:shipping_address",
        "transforms.dropField.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
        "transforms.dropField.blacklist": "CREATE_TS"
      }
    }'
----


Inspect the data in Elasticsearch: 

[source,bash]
----
curl -s http://localhost:9200/_cat/indices
----

[source,bash]
----
green  open .kibana_task_manager              AhFACVWpRby6kZwYFwM68w 1 0    2 0 12.5kb 12.5kb
green  open .kibana_1                         xTC-RMxZSj-KcF22zmEoZA 1 0    5 0 22.9kb 22.9kb
yellow open asgard.demo.orders-201904         qzMvZH8DQWKkLjr1yFB-Bw 5 1 3338 0  1.3mb  1.3mb
yellow open mysql-debezium-asgard.demo.orders l5dwQAfjRkWfhTP7EZRFrw 5 1    0 0  1.2kb  1.2kb
----


[source,bash]
----
echo '{ "size": 1, "sort": [ { "UPDATE_TS": { "order": "desc" } } ] }' |\
  http http://localhost:9200/asgard.demo.orders-201904/_search
----

or

[source,bash]
----
curl -s http://localhost:9200/asgard.demo.orders-201904/_search \
  -H 'content-type: application/json' \
  -d '{ "size": 1, "sort": [ { "UPDATE_TS": { "order": "desc" } } ] }' |\
  jq '.'
----
