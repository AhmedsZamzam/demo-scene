= Streaming ETL demo - Enriching event stream data with CDC data from MySQL, stream into Elasticsearch
Robin Moffatt <robin@confluent.io>
v1.30, October 30, 2018

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

This is designed to be run as a step-by-step demo. The `ksql-statements.sql` should match those run in this doc end-to-end and in theory you can just run the file, but I have not tested it. PRs welcome for a one-click script that just demonstrates the end-to-end running demo :)

The slides that accompany this demo can be found here: https://speakerdeck.com/rmoff/apache-kafka-and-ksql-in-action-lets-build-a-streaming-data-pipeline

== Pre-reqs

Local:

* `curl`
* `jq`
* Docker

== Pre-Flight Setup

Start the environment

[source,bash]
----
cd docker-compose
./scripts/setup.sh
----

=== Run KSQL CLI and MySQL CLI

Optionally, use something like `screen` or `tmux` to have these both easily to hand. Or multiple Terminal tabs. Whatever works for you :)

* KSQL CLI:
+
[source,bash]
----
docker run --network docker-compose_default --interactive --tty --rm \
     confluentinc/cp-ksql-cli:5.0.0 \
     http://ksql-server:8088
----

* MySQL CLI:
+
[source,bash]
----
cd docker-compose
docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD demo'
----

* kafkacat:
+
[source,bash]
----
docker run --rm -it --network docker-compose_default confluentinc/cp-kafkacat \
  kafkacat -b kafka:29092 -t lisa18 -P
----

== Pre-flight checklist

* Load Kibana dashboard: http://localhost:5601/app/kibana#/dashboard/mysql-ksql-kafka-es
* Launch http://localhost:5601/app/kibana#/discover?_g=(refreshInterval:('$$hashKey':'object:315',display:'1%20seconds',pause:!f,section:1,value:1000),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:lisa18,interval:auto,query:(language:lucene,query:''),sort:!(_score,desc))[Kibana dashboard]
* Create iTerm window, using the `screencapture` profile
* Load this instructions doc into Chrome
* Close all other apps
* Optional: 
** Make sure phone is on wifi/internet
** `cd ios_push_notifications`
** `python push_bullet.py`
** Connect laptop to wifi
** Connect iPhone to laptop with USB
** Launch QuickTime Player, select iPhone as source
** Disable other notifications (WhatsApp/Messages/Slack etc)

== Demo

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

== Part 01 - Kafka Connect

=== Kafka to Elasticsearch

Show Kafka Connect config

[source,json]
----
curl -X "POST" "http://kafka-connect-cp:18083/connectors/" \
     -H "Content-Type: application/json" \
     -d '{
    "name": "es_sink_lisa18",
    "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": false,
    "topics": "lisa18",
    "key.ignore": "true",
    "schema.ignore": "true",
    "type.name": "type.name=kafkaconnect",
    "connection.url": "http://elasticsearch:9200"
  }
}'
----

Launch http://localhost:5601/app/kibana#/discover?_g=(refreshInterval:('$$hashKey':'object:315',display:'1%20seconds',pause:!f,section:1,value:1000),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:lisa18,interval:auto,query:(language:lucene,query:''),sort:!(_score,desc))[Kibana dashboard]

From kafkacat prompt

[source,bash]
----
docker run --rm -it --network docker-compose_default confluentinc/cp-kafkacat \
  kafkacat -b kafka:29092 -t lisa18 -P
----

send some data - after each observe that dashboard reflects new data straight away

[source,bash]
----
{"hello":"world"}
{"hello":"lisa18"}
{"lisa18":"is great!"}
----



=== Show MySQL table + contents

[source,sql]
----
mysql> show tables;
+----------------+
| Tables_in_demo |
+----------------+
| CUSTOMERS      |
+----------------+
1 row in set (0.00 sec)

mysql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS LIMIT 5;
+----+-------------+------------+------------------------+-------------+
| ID | FIRST_NAME  | LAST_NAME  | EMAIL                  | CLUB_STATUS |
+----+-------------+------------+------------------------+-------------+
|  1 | Rica        | Blaisdell  | rblaisdell0@rambler.ru | bronze      |
|  2 | Ruthie      | Brockherst | rbrockherst1@ow.ly     | platinum    |
|  3 | Mariejeanne | Cocci      | mcocci2@techcrunch.com | bronze      |
|  4 | Hashim      | Rumke      | hrumke3@sohu.com       | platinum    |
|  5 | Hansiain    | Coda       | hcoda4@senate.gov      | platinum    |
+----+-------------+------------+------------------------+-------------+
5 rows in set (0.00 sec)
----

=== Check status of Debezium connectors

[source,bash]
----
curl -s "http://localhost:8083/connectors"| jq '.[]'| xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| column -s : -t| sed 's/\"//g'| sort
mysql-source-demo-CUSTOMERS      |  RUNNING  |  RUNNING
mysql-source-demo-CUSTOMERS-raw  |  RUNNING  |  RUNNING
----

=== Show Kafka topic has been created & populated

_You can do this with KSQL and `PRINT 'asgard.demo.accounts' FROM BEGINNING;` instead of console tools._

Show contents:

[source,bash]
----
$ docker-compose exec -T kafka-connect-cp \
                 kafka-avro-console-consumer \
                  --bootstrap-server kafka:29092 \
                  --property schema.registry.url=http://schema-registry:8081 \
                  --topic asgard.demo.CUSTOMERS \
                  --from-beginning \
                  | jq '.'
{
  "id": 1,
  "first_name": {
    "string": "Bibby"
  },
  "last_name": {
    "string": "Argabrite"
  },
  "email": {
    "string": "bargabrite0@google.com.hk"
  },
  "gender": {
    "string": "Female"
  },
  "comments": {
    "string": "Reactive exuding productivity"
  },
  "messagetopic": {
    "string": "asgard.demo.CUSTOMERS"
  },
  "messagesource": {
    "string": "Debezium CDC from MySQL on asgard"
  }
}
----

=== Show CDC in action with before/after record data

[source,bash]
----
docker-compose exec -T kafka-connect-cp \
 kafka-avro-console-consumer \
 --bootstrap-server kafka:29092 \
 --property schema.registry.url=http://schema-registry:8081 \
 --topic asgard.demo.CUSTOMERS-raw --from-beginning  | jq  '.'
----

==== Insert a row in MySQL, observe it in Kafka

If not running the console consumer, then run `PRINT 'asgard.demo.CUSTOMERS' FROM BEGINNING;` to see the topic contents and and new messages. 

[source,sql]
----
insert into CUSTOMERS (id,first_name,last_name) values (42,'Rick','Astley');
----

==== Update a row in MySQL, observe it in Kafka

[source,sql]
----
update CUSTOMERS set first_name='Bob' where id=1;
----

Point out before/after records in `raw` stream

==== Optional Delete a row in MySQL, observe it in Kafka

[source,sql]
----
DELETE FROM CUSTOMERS WHERE ID=8;
----

Point out before/after records in `raw` stream

---

Return to slides 

---

== Part 02 - KSQL for filtering streams

=== Inspect topics

[source,sql]
----
SHOW TOPICS;
----

=== Inspect ratings & define stream

[source,sql]
----
PRINT 'ratings';
CREATE STREAM RATINGS WITH (KAFKA_TOPIC='ratings',VALUE_FORMAT='AVRO');
----

=== Filter live stream of data

[source,sql]
----
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;
----

=== Create a derived stream

[source,sql]
----
CREATE STREAM POOR_REVIEWS AS \
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;
----

---

Return to slides 

---

== Part 03 - KSQL for joining streams

=== Inspect CUSTOMERS data
[source,sql]
----
-- Inspect raw topic data if you want
-- PRINT 'asgard.demo.CUSTOMERS' FROM BEGINNING;

CREATE STREAM CUSTOMERS_SRC WITH (KAFKA_TOPIC='asgard.demo.CUSTOMERS', VALUE_FORMAT='AVRO');
SET 'auto.offset.reset' = 'earliest';
SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_SRC WHERE ID=1;
----

=== Re-key the customer data
[source,sql]
----
SET 'auto.offset.reset' = 'earliest';
CREATE STREAM CUSTOMERS_SRC_REKEY WITH (PARTITIONS=1) AS SELECT * FROM CUSTOMERS_SRC PARTITION BY ID;
-- Wait for a moment here; if you run the CTAS _immediately_ after the CSAS it may fail
-- with error `Could not fetch the AVRO schema from schema registry. Subject not found.; error code: 40401`
-- You may also get this error if you have not set 'auto.offset.reset'='earliest' and there is no 
-- data flowing into the source CUSTOMERS topic, since no messages will have triggered the target stream 
-- to be created.
-- See https://github.com/confluentinc/ksql/issues/713
CREATE TABLE CUSTOMERS WITH (KAFKA_TOPIC='CUSTOMERS_SRC_REKEY', VALUE_FORMAT ='AVRO', KEY='ID');
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS WHERE ID=1;
----

==== [Optional] Demonstrate Stream / Table difference

Here's the stream - every event, which in this context is every change event on the source database: 

[source,sql]
----
ksql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS_SRC WHERE ID=1;
1 | Rica | Blaisdell | rblaisdell0@rambler.ru | bronze
1 | Bob | Blaisdell | rblaisdell0@rambler.ru | bronze
1 | Fred | Blaisdell | rblaisdell0@rambler.ru | bronze
^CQuery terminated
ksql>
----

Here's the table - the latest value for a given key
[source,sql]
----
ksql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS WHERE ID=1;
1 | Fred | Blaisdell | rblaisdell0@rambler.ru | bronze
^CQuery terminated
----

==== [Optional] Demonstrate why the re-key is required

[source,sql]
----
ksql> SELECT C.ROWKEY, C.ID FROM CUSTOMERS_SRC C LIMIT 3;
 | 1
 | 2
 | 3

ksql> SELECT C.ROWKEY, C.ID FROM CUSTOMERS C LIMIT 3;
1 | 1
2 | 2
3 | 3
----


=== Join live stream of ratings to customer data

[source,sql]
----
ksql> SELECT R.RATING_ID, R.MESSAGE, \
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, \
      C.CLUB_STATUS \
      FROM RATINGS R \
        LEFT JOIN CUSTOMERS C \
        ON R.USER_ID = C.ID \
      WHERE C.FIRST_NAME IS NOT NULL;
524 | Surprisingly good, maybe you are getting your mojo back at long last! | Patti Rosten | silver
525 | meh | Fred Blaisdell | bronze
526 | more peanuts please | Hashim Rumke | platinum
527 | more peanuts please | Laney Toopin | platinum
529 | Exceeded all my expectations. Thank you ! | Ruthie Brockherst | platinum
530 | (expletive deleted) | Brianna Paradise | bronze
…
----

Persist this stream of data

[source,sql]
----
CREATE STREAM RATINGS_WITH_CUSTOMER_DATA \
       WITH (PARTITIONS=1, \
             KAFKA_TOPIC='ratings-enriched') \
       AS \
SELECT R.RATING_ID, R.MESSAGE, R.STARS, R.CHANNEL,\
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, \
      C.CLUB_STATUS, C.EMAIL \
      FROM RATINGS R \
        LEFT JOIN CUSTOMERS C \
        ON R.USER_ID = C.ID \
      WHERE C.FIRST_NAME IS NOT NULL;
----

The `WITH (PARTITIONS=1)` is only necessary if the Elasticsearch connector has already been defined, as it will create the topic before KSQL does, and using a single partition (not 4, as KSQL wants to by default).

=== Examine changing reference data

CUSTOMERS is a KSQL _table_, which means that we have the latest value for a given key.

Check out the ratings for customer id 2 only:
[source,sql]
----
ksql> SELECT * FROM RATINGS_WITH_CUSTOMER_DATA WHERE ID=2;
----

In mysql, make a change to ID 2
[source,sql]
----
mysql> UPDATE CUSTOMERS SET FIRST_NAME = 'Thomas', LAST_NAME ='Smith' WHERE ID=2;
----

Observe in the continuous KSQL query that the customer name has now changed.

=== Create stream of unhappy VIPs

[source,sql]
----
CREATE STREAM UNHAPPY_PLATINUM_CUSTOMERS \
       WITH (VALUE_FORMAT='JSON', PARTITIONS=1) AS \
SELECT FULL_NAME, CLUB_STATUS, EMAIL, STARS, MESSAGE \
FROM   RATINGS_WITH_CUSTOMER_DATA \
WHERE  STARS < 3 \
  AND  CLUB_STATUS = 'platinum';
----

== View in Elasticsearch and Kibana

Tested on Elasticsearch 6.3.0

image:images/es01.png[Kibana]

---

Return to slides 

---

#EOF

== Optional


=== Aggregations

Simple aggregation - count of ratings per person, per minute:

[source,sql]
----
ksql> SELECT FULL_NAME,COUNT(*) FROM RATINGS_WITH_CUSTOMER_DATA WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
----

Persist this and show the timestamp:

[source,sql]
----
CREATE TABLE RATINGS_PER_CUSTOMER_PER_MINUTE AS SELECT FULL_NAME,COUNT(*) AS RATINGS_COUNT FROM ratings_with_customer_data WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') , FULL_NAME, RATINGS_COUNT FROM RATINGS_PER_CUSTOMER_PER_MINUTE;
----

=== Slack/PushBullet notifications

_This bit will need some config of your own, as you'll need your own Slack workspace and API key (both free). With this though, you can demo the idea of an event-driven app subscribing to a KSQL-populated stream of filtered events._

_A newer version of the push notification script uses PushBullet, see `ios_push_notifications/push_bullet.py`._

image:images/slack_ratings.png[Slack push notifications driven from Kafka and KSQL]

To run, first export your API key as an environment variable:

[source,bash]
----
export SLACK_API_TOKEN=xyxyxyxyxyxyxyxyxyxyxyx
----

Or if you've got it locally, run `source slack_creds.sh`

then run the code:

[source,bash]
----
python python_kafka_notify.py
----

You will need to install `slackclient` and `confluent_kafka` libraries.

