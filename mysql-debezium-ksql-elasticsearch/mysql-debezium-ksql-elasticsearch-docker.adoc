= Streaming ETL demo - Enriching event stream data with CDC data from MySQL, stream into Elasticsearch
Robin Moffatt <robin@confluent.io>
v1.15, 25 Jul 2018

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

This is designed to be run as a step-by-step demo. The `ksql-statements.sql` should match those run in this doc end-to-end and in theory you can just run the file, but I have not tested it. PRs welcome for a one-click script that just demonstrates the end-to-end running demo :)

The slides that accompany this demo can be found here: https://speakerdeck.com/rmoff/apache-kafka-and-ksql-in-action-lets-build-a-streaming-data-pipeline

== Pre-reqs

Local:

* `curl`
* `jq`
* Docker

== Pre-Flight Setup

Start the environment

[source,bash]
----
cd docker-compose
docker-compose up -d
docker-compose logs -f kafka-connect-cp|grep "Kafka Connect started"
----

[source,bash]
----
./scripts/setup.sh
----


=== Setup Kibana

1. Go to http://localhost:5601/app/kibana#/management/kibana/indices/ and set an index (any index) as default

2. Go to http://localhost:5601/app/kibana#/management/kibana/objects and Import dashboard `kibana_objects.json` using the **Import** button 
+
image::images/kibana_ix01.png[Kibana indexes]

3. Load the Kibana dashboard http://localhost:5601/app/kibana#/dashboard/02028f30-424c-11e8-af19-1188a9332246


=== Run KSQL CLI and MySQL CLI

Optionally, use something like `screen` or `tmux` to have these both easily to hand. Or multiple Terminal tabs. Whatever works for you :)
KSQL CLI:

[source,bash]
----
docker run --network docker-compose_default --interactive --tty \
     confluentinc/cp-ksql-cli:5.0.0 \
     http://ksql-server:8088
----

MySQL CLI:

[source,bash]
----
cd docker-compose
docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD demo'
----

== Pre-flight checklist

* Load Kibana dashboard: http://localhost:5601/app/kibana#/dashboard/02028f30-424c-11e8-af19-1188a9332246
* Create iTerm window, using the `screencapture` profile
* Load this instructions doc into Chrome
* Close all other apps

== Demo

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

== Part 01 - Kafka Connect

=== Show MySQL table + contents

[source,sql]
----
mysql> show tables;
+----------------+
| Tables_in_demo |
+----------------+
| CUSTOMERS      |
+----------------+
1 row in set (0.00 sec)

mysql> select id, first_name, last_name, club_status from CUSTOMERS;
+----+-------------+------------+-------------+
| id | first_name  | last_name  | club_status |
+----+-------------+------------+-------------+
|  1 | Bob         | Blaisdell  | bronze      |
|  2 | Ruthie      | Brockherst | platinum    |
|  3 | Mariejeanne | Cocci      | bronze      |
|  4 | Hashim      | Rumke      | platinum    |
|  5 | Hansiain    | Coda       | platinum    |
|  6 | Robinet     | Leheude    | platinum    |
|  7 | Fay         | Huc        | bronze      |
|  8 | Patti       | Rosten     | silver      |
…
----

=== Check status of Debezium connectors

[source,bash]
----
curl -s "http://localhost:8083/connectors"| jq '.[]'| xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| column -s : -t| sed 's/\"//g'| sort
mysql-source-demo-CUSTOMERS      |  RUNNING  |  RUNNING
mysql-source-demo-CUSTOMERS-raw  |  RUNNING  |  RUNNING
----

=== Show Kafka topic has been created & populated

Show contents:

[source,bash]
----
$ docker-compose exec -T kafka-connect-cp \
                 kafka-avro-console-consumer \
                  --bootstrap-server kafka:29092 \
                  --property schema.registry.url=http://schema-registry:8081 \
                  --topic asgard.demo.CUSTOMERS \
                  --from-beginning \
                  | jq '.'
{
  "id": 1,
  "first_name": {
    "string": "Bibby"
  },
  "last_name": {
    "string": "Argabrite"
  },
  "email": {
    "string": "bargabrite0@google.com.hk"
  },
  "gender": {
    "string": "Female"
  },
  "comments": {
    "string": "Reactive exuding productivity"
  },
  "messagetopic": {
    "string": "asgard.demo.CUSTOMERS"
  },
  "messagesource": {
    "string": "Debezium CDC from MySQL on asgard"
  }
}
----

=== Show CDC in action with before/after record data

[source,bash]
----
docker-compose exec -T kafka-connect-cp \
 kafka-avro-console-consumer \
 --bootstrap-server kafka:29092 \
 --property schema.registry.url=http://schema-registry:8081 \
 --topic asgard.demo.CUSTOMERS-raw --from-beginning  | jq  '.'
----

==== Insert a row in MySQL, observe it in Kafka

If not running the console consumer, then run `PRINT 'asgard.demo.CUSTOMERS' FROM BEGINNING;` to see the topic contents and and new messages. 

[source,sql]
----
insert into CUSTOMERS (id,first_name,last_name) values (42,'Rick','Astley');
----

==== Update a row in MySQL, observe it in Kafka

[source,sql]
----
update CUSTOMERS set first_name='Bob' where id=1;
----

Point out before/after records in `raw` stream

==== Optional Delete a row in MySQL, observe it in Kafka

[source,sql]
----
DELETE FROM CUSTOMERS WHERE ID=8;
----

Point out before/after records in `raw` stream

---

Return to slides 

---

== Part 02 - KSQL for filtering streams

=== Inspect topics

[source,sql]
----
SHOW TOPICS;
----

=== Inspect ratings & define stream

[source,sql]
----
PRINT 'ratings';
CREATE STREAM RATINGS WITH (KAFKA_TOPIC='ratings',VALUE_FORMAT='AVRO');
----

=== Filter live stream of data

[source,sql]
----
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;
----

=== Create a derived stream

[source,sql]
----
CREATE STREAM POOR_REVIEWS AS \
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;
----

---

Return to slides 

---

== Part 03 - KSQL for joining streams

=== Inspect CUSTOMERS data
[source,sql]
----
-- Inspect raw topic data if you want
-- PRINT 'asgard.demo.CUSTOMERS' FROM BEGINNING;

CREATE STREAM CUSTOMERS_SRC WITH (KAFKA_TOPIC='asgard.demo.CUSTOMERS', VALUE_FORMAT='AVRO');
SET 'auto.offset.reset' = 'earliest';
SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_SRC;
----

=== Re-key the customer data
[source,sql]
----
CREATE STREAM CUSTOMERS_SRC_REKEY WITH (PARTITIONS=1) AS SELECT * FROM CUSTOMERS_SRC PARTITION BY ID;
-- Wait for a moment here; if you run the CTAS _immediately_ after the CSAS it may fail
-- with error `Could not fetch the AVRO schema from schema registry. Subject not found.; error code: 40401`
CREATE TABLE CUSTOMERS WITH (KAFKA_TOPIC='CUSTOMERS_SRC_REKEY', VALUE_FORMAT ='AVRO', KEY='ID');
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS;
----

==== [Optional] Demonstrate Stream / Table difference

Here's the stream - every event, which in this context is every change event on the source database: 

[source,sql]
----
ksql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS_SRC WHERE ID=1;
1 | Rica | Blaisdell | rblaisdell0@rambler.ru | bronze
1 | Bob | Blaisdell | rblaisdell0@rambler.ru | bronze
1 | Fred | Blaisdell | rblaisdell0@rambler.ru | bronze
^CQuery terminated
ksql>
----

Here's the table - the latest value for a given key
[source,sql]
----
ksql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS WHERE ID=1;
1 | Fred | Blaisdell | rblaisdell0@rambler.ru | bronze
^CQuery terminated
----

==== [Optional] Demonstrate why the re-key is required

[source,sql]
----
ksql> SELECT C.ROWKEY, C.ID FROM CUSTOMERS_SRC C LIMIT 3;
 | 1
 | 2
 | 3

ksql> SELECT C.ROWKEY, C.ID FROM CUSTOMERS C LIMIT 3;
1 | 1
2 | 2
3 | 3
----


=== Join live stream of ratings to customer data

[source,sql]
----
ksql> SELECT R.RATING_ID, R.MESSAGE, \
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, \
      C.CLUB_STATUS \
      FROM RATINGS R \
        LEFT JOIN CUSTOMERS C \
        ON R.USER_ID = C.ID \
      WHERE C.FIRST_NAME IS NOT NULL;
524 | Surprisingly good, maybe you are getting your mojo back at long last! | Patti Rosten | silver
525 | meh | Fred Blaisdell | bronze
526 | more peanuts please | Hashim Rumke | platinum
527 | more peanuts please | Laney Toopin | platinum
529 | Exceeded all my expectations. Thank you ! | Ruthie Brockherst | platinum
530 | (expletive deleted) | Brianna Paradise | bronze
…
----

Persist this stream of data

[source,sql]
----
CREATE STREAM RATINGS_WITH_CUSTOMER_DATA \
       WITH (PARTITIONS=1) AS \
SELECT R.RATING_ID, R.MESSAGE, R.STARS, R.CHANNEL,\
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, \
      C.CLUB_STATUS, C.EMAIL \
      FROM RATINGS R \
        LEFT JOIN CUSTOMERS C \
        ON R.USER_ID = C.ID \
      WHERE C.FIRST_NAME IS NOT NULL;
----

The `WITH (PARTITIONS=1)` is only necessary if the Elasticsearch connector has already been defined, as it will create the topic before KSQL does, and using a single partition (not 4, as KSQL wants to by default).

=== Examine changing reference data

CUSTOMERS is a KSQL _table_, which means that we have the latest value for a given key.

Check out the ratings for customer id 2 only:
[source,sql]
----
ksql> SELECT * FROM RATINGS_WITH_CUSTOMER_DATA WHERE ID=2;
----

In mysql, make a change to ID 2
[source,sql]
----
mysql> UPDATE CUSTOMERS SET FIRST_NAME = 'Thomas', LAST_NAME ='Smith' WHERE ID=2;
----

Observe in the continuous KSQL query that the customer name has now changed.

=== Create stream of unhappy VIPs

[source,sql]
----
CREATE STREAM UNHAPPY_PLATINUM_CUSTOMERS \
       WITH (VALUE_FORMAT='JSON', PARTITIONS=1) AS \
SELECT FULL_NAME, CLUB_STATUS, EMAIL, STARS, MESSAGE \
FROM   RATINGS_WITH_CUSTOMER_DATA \
WHERE  STARS < 3 \
  AND  CLUB_STATUS = 'platinum';
----

== View in Elasticsearch and Kibana

Tested on Elasticsearch 6.3.0

image:images/es01.png[Kibana]

---

Return to slides 

---

#EOF

== Optional


=== Aggregations

Simple aggregation - count of ratings per person, per minute:

[source,sql]
----
ksql> SELECT FULL_NAME,COUNT(*) FROM RATINGS_WITH_CUSTOMER_DATA WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
----

Persist this and show the timestamp:

[source,sql]
----
CREATE TABLE RATINGS_PER_CUSTOMER_PER_MINUTE AS SELECT FULL_NAME,COUNT(*) AS RATINGS_COUNT FROM ratings_with_customer_data WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') , FULL_NAME, RATINGS_COUNT FROM RATINGS_PER_CUSTOMER_PER_MINUTE;
----

=== Slack notifications

_This bit will need some config of your own, as you'll need your own Slack workspace and API key (both free). With this though, you can demo the idea of an event-driven app subscribing to a KSQL-populated stream of filtered events.

:image:images/slack_ratings.png[Slack push notifications driven from Kafka and KSQL]

To run, first export your API key as an environment variable:

[source,bash]
----
export SLACK_API_TOKEN=xyxyxyxyxyxyxyxyxyxyxyx
----

then run the code:

[source,bash]
----
python python_kafka_notify.py
----

You will need to install `slackclient` and `confluent_kafka` libraries.
