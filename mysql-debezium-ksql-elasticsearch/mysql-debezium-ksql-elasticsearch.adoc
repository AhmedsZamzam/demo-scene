= MySQL / Debezium / KSQL / Elasticsearch demo

== Prep

=== Start Confluent platform

[source,bash]
----
confluent destroy
confluent start
----

=== Prep MySQL

Assumes it's installed locally

Drop & repopulate data:

[source,bash]
----
mysql demo -uroot < ~/git/demo-scene/mysql-debezium-ksql-elasticsearch/customers.sql
----

=== Setup screen

[source,bash]
----
screen -d -m -S CP_demo -t bash watch date
screen -S CP_demo -p 0 -X screen -t Elasticsearch elasticsearch
screen -S CP_demo -p 0 -X screen -t Kibana kibana
screen -S CP_demo -p 0 -X screen -t mysql mysql -uroot demo
#screen -S CP_demo -p 0 -X screen -t topic-raw kafka-avro-console-consumer --bootstrap-server localhost:9092 --property schema.registry.url=http://localhost:8081 --topic asgard.demo.customers-raw --from-beginning  | jq '.'
#screen -S CP_demo -p 0 -X screen -t topic-flat kafka-avro-console-consumer --bootstrap-server localhost:9092 --property schema.registry.url=http://localhost:8081 --topic asgard.demo.customers --from-beginning  | jq '.'
screen -S CP_demo -p 0 -X screen -t ksql ksql
screen -S CP_demo -p 0 -X screen -t datagen ksql-datagen quickstart=ratings format=avro topic=ratings maxInterval=500
----

=== Start ratings datagen (if not already in `screen`)

`ksql-datagen` is in the Confluent Platform `bin` folder, which should already be on your `PATH`.

[source,bash]
----
ksql-datagen quickstart=ratings format=avro topic=ratings maxInterval=500
----

=== Create CDC connectors (one raw, one flattened)

Assumes https://rmoff.net/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/[Debezium plugin is already installed]

[source,bash]
----
curl -i -X POST -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/ \
    -d '{
      "name": "mysql-source-demo-customers",
      "config": {
            "connector.class": "io.debezium.connector.mysql.MySqlConnector",
            "database.hostname": "localhost",
            "database.port": "3306",
            "database.user": "debezium",
            "database.password": "dbz",
            "database.server.id": "42",
            "database.server.name": "asgard",
            "table.whitelist": "demo.customers",
            "database.history.kafka.bootstrap.servers": "localhost:9092",
            "database.history.kafka.topic": "dbhistory.demo" ,
            "include.schema.changes": "true",
            "transforms": "unwrap,InsertTopic,InsertSourceDetails",
            "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
            "transforms.InsertTopic.type":"org.apache.kafka.connect.transforms.InsertField$Value",
            "transforms.InsertTopic.topic.field":"messagetopic",
            "transforms.InsertSourceDetails.type":"org.apache.kafka.connect.transforms.InsertField$Value",
            "transforms.InsertSourceDetails.static.field":"messagesource",
            "transforms.InsertSourceDetails.static.value":"Debezium CDC from MySQL on asgard"
       }
    }'
----

[source,bash]
----
curl -i -X POST -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/ \
    -d '{
      "name": "mysql-source-demo-customers-raw",
      "config": {
            "connector.class": "io.debezium.connector.mysql.MySqlConnector",
            "database.hostname": "localhost",
            "database.port": "3306",
            "database.user": "debezium",
            "database.password": "dbz",
            "database.server.id": "43",
            "database.server.name": "asgard",
            "table.whitelist": "demo.customers",
            "database.history.kafka.bootstrap.servers": "localhost:9092",
            "database.history.kafka.topic": "dbhistory.demo" ,
            "include.schema.changes": "true",
            "transforms": "addTopicSuffix",
            "transforms.addTopicSuffix.type":"org.apache.kafka.connect.transforms.RegexRouter",
            "transforms.addTopicSuffix.regex":"(.*)",
            "transforms.addTopicSuffix.replacement":"$1-raw"       }
    }'
----

=== Prep Elasticsearch and Kibana

==== Run Elasticsearch and Kibana if not already in `screen`

[source,bash]
----
elasticsearch
kibana
----

If not installed then:

[source,bash]
----
brew install elasticsearch
brew install kibana
----

==== Set up Elasticsearch dynamic mapping template


[source,bash]
----
curl -XPUT "http://localhost:9200/_template/kafkaconnect/" -H 'Content-Type: application/json' -d'
{
  "index_patterns": "*",
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "dates": {
            "match": "EXTRACT_TS",
            "mapping": {
              "type": "date"
            }
          }
        },
        {
          "non_analysed_string_template": {
            "match": "*",
            "match_mapping_type": "string",
            "mapping": {
              "type": "keyword"
            }
          }
        }
      ]
    }
  }
}'
----

==== Set up Elasticsearch Kafka Connect connector

[source,bash]
----
curl -X "POST" "http://localhost:8083/connectors/" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "es_sink_RATINGS_ENRICHED",
  "config": {
    "topics": "'RATINGS_ENRICHED'",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "key.ignore": "true",
    "schema.ignore": "false",
    "type.name": "type.name=kafkaconnect",
    "topic.index.map": "'RATINGS_ENRICHED':'ratings_enriched'",
    "connection.url": "http://localhost:9200",
    "transforms": "ExtractTimestamp",
    "transforms.ExtractTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.ExtractTimestamp.timestamp.field" : "EXTRACT_TS"
  }
}'
----

== Pre-flight checklist

Is the stack up?

[source,bash]
----
Robin@asgard02 ~> confluent status
control-center is [UP]
ksql-server is [UP]
connect is [UP]
kafka-rest is [UP]
schema-registry is [UP]
kafka is [UP]
zookeeper is [UP]
Robin@asgard02 ~>
----

Are the connectors running?

[source,bash]
----
Robin@asgard02 ~> confluent status connectors|grep -v Writing| jq '.[]'|  xargs -I{connector} confluent status {connector}|  grep -v Writing| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'|  column -s : -t|  sed 's/\"//g'|  sort
es_sink_RATINGS_ENRICHED        |  RUNNING  |  RUNNING
mysql-source-demo-customers      |  RUNNING  |  RUNNING
mysql-source-demo-customers-raw  |  RUNNING  |  RUNNING
----

Is ratings data being produced?

[source,bash]
----
Robin@asgard02 ~> kafka-avro-console-consumer \
                  --bootstrap-server localhost:9092 \
                  --property schema.registry.url=http://localhost:8081 \
                  --topic ratings
{"rating_id":{"long":2253},"user_id":{"int":14},"stars":{"int":4},"route_id":{"int":1955},"rating_time":{"long":1523986139221},"channel":{"string":"ios"},"message":{"string":"Exceeded all my expectations. Thank you !"}}
----

Is Elasticsearch running?

[source,bash]
----
Robin@asgard02 ~> curl http://localhost:9200
{
  "name" : "0-JgLQj",
  "cluster_name" : "elasticsearch_Robin",
  "cluster_uuid" : "XKkAsum3QL-ECyZlP8z-rA",
  "version" : {
    "number" : "6.2.3",
    "build_hash" : "c59ff00",
    "build_date" : "2018-03-13T10:06:29.741383Z",
    "build_snapshot" : false,
    "lucene_version" : "7.2.1",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
----

* Load Kibana : http://localhost:5601/app/kibana#/
* Create two iTerm windows, using the `screencapture` profile
* Load file:///Users/Robin/git/demo-scene/mysql-debezium-ksql-elasticsearch/mysql-debezium-ksql-elasticsearch.adoc into Chrome
* Close all other apps

== Demo

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

=== Inspect topics

[source,sql]
----
SHOW TOPICS;
----

=== Inspect ratings & define stream

[source,sql]
----
PRINT 'ratings';
CREATE STREAM RATINGS WITH (KAFKA_TOPIC='ratings',VALUE_FORMAT='AVRO');
----

=== Filter live stream of data

[source,sql]
----
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS=1;
----

=== Show MySQL table + contents

[source,sql]
----
mysql> show tables;
+----------------+
| Tables_in_demo |
+----------------+
| customers      |
+----------------+
1 row in set (0.00 sec)

mysql> select * from customers;
+----+------------+-----------+--------------------------------+--------+------------------------------------------------------+
| id | first_name | last_name | email                          | gender | comments                                             |
+----+------------+-----------+--------------------------------+--------+------------------------------------------------------+
|  1 | Bibby      | Argabrite | bargabrite0@google.com.hk      | Female | Reactive exuding productivity                        |
|  2 | Auberon    | Sulland   | asulland1@slideshare.net       | Male   | Organized context-sensitive Graphical User Interface |
|  3 | Marv       | Dalrymple | mdalrymple2@macromedia.com     | Male   | Versatile didactic pricing structure                 |
|  4 | Nolana     | Yeeles    | nyeeles3@drupal.org            | Female | Adaptive real-time archive                           |
|  5 | Modestia   | Coltart   | mcoltart4@scribd.com           | Female | Reverse-engineered non-volatile success              |
|  6 | Bram       | Acaster   | bacaster5@pagesperso-orange.fr | Male   | Robust systematic support                            |
|  7 | Marigold   | Veld      | mveld6@pinterest.com           | Female | Sharable logistical installation                     |
|  8 | Ruperto    | Matteotti | rmatteotti7@diigo.com          | Male   | Diverse client-server conglomeration                 |
+----+------------+-----------+--------------------------------+--------+------------------------------------------------------+
8 rows in set (0.00 sec)
----

=== Check status of connectors

[source,bash]
----
confluent status connectors|grep -v Writing| jq '.[]'|  xargs -I{connector} confluent status {connector}|  grep -v Writing| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'|  column -s : -t|  sed 's/\"//g'|  sort
mysql-source-demo-customers  |  RUNNING  |  RUNNING
mysql-source-demo-customers-raw  |  RUNNING  |  RUNNING
----

=== Show Kafka topic has been created & populated

[Optional - show that the topic has been created] `-v` to exclude system topics prefixed with `_`

[source,bash]
----
$ kafka-topics --zookeeper localhost:2181 --list|grep -v _
asgard
asgard.demo.customers
connect-configs
connect-offsets
connect-statuses
dbhistory.demo
----

Show contents:

[source,bash]
----
$ kafka-avro-console-consumer \
   --bootstrap-server localhost:9092 \
   --property schema.registry.url=http://localhost:8081 \
   --topic asgard.demo.customers --from-beginning --max-messages=1 | jq '.'
{
  "id": 1,
  "first_name": {
    "string": "Bibby"
  },
  "last_name": {
    "string": "Argabrite"
  },
  "email": {
    "string": "bargabrite0@google.com.hk"
  },
  "gender": {
    "string": "Female"
  },
  "comments": {
    "string": "Reactive exuding productivity"
  },
  "messagetopic": {
    "string": "asgard.demo.customers"
  },
  "messagesource": {
    "string": "Debezium CDC from MySQL on asgard"
  }
}
Processed a total of 1 messages
----

=== Show CDC in action

Run consumer, one for raw, one for flattened :

[source,bash]
----
kafka-avro-console-consumer \
 --bootstrap-server localhost:9092 \
 --property schema.registry.url=http://localhost:8081 \
 --topic asgard.demo.customers --from-beginning  | jq  '.'
----

[source,bash]
----
kafka-avro-console-consumer \
 --bootstrap-server localhost:9092 \
 --property schema.registry.url=http://localhost:8081 \
 --topic asgard.demo.customers-raw --from-beginning  | jq '.'
----

==== Insert a row in MySQL, observe it in Kafka

[source,sql]
----
insert into customers (id,first_name,last_name) values (42,'Rick','Astley');
----

==== Update a row in MySQL, observe it in Kafka

[source,sql]
----
update customers set first_name='Bob' where id=1;
----

Point out before/after records in `raw` stream

==== Delete a row in MySQL, observe it in Kafka

[source,sql]
----
DELETE FROM customers WHERE ID=8;
----

Point out before/after records in `raw` stream

=== Inspect customers data
[source,sql]
----
PRINT 'asgard.demo.customers' FROM BEGINNING;

CREATE STREAM CUSTOMERS_SRC WITH (KAFKA_TOPIC='asgard.demo.customers', VALUE_FORMAT='AVRO');
SET 'auto.offset.reset' = 'earliest';
SELECT ID, FIRST_NAME, LAST_NAME FROM CUSTOMERS_SRC;
----

=== Re-key the customer data
[source,sql]
----
CREATE STREAM CUSTOMERS_SRC_REKEY AS SELECT * FROM CUSTOMERS_SRC PARTITION BY ID;
-- Wait for a moment here; if you run the CTAS _immediately_ after the CSAS it may fail
-- with error `Could not fetch the AVRO schema from schema registry. Subject not found.; error code: 40401`
CREATE TABLE CUSTOMERS WITH (KAFKA_TOPIC='CUSTOMERS_SRC_REKEY', VALUE_FORMAT ='AVRO', KEY='ID');
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, MESSAGESOURCE FROM CUSTOMERS;
----

==== [Optional] Demonstrate why the re-key is required

[source,sql]
----
ksql> SELECT C.ROWKEY, C.ID FROM CUSTOMERS_SRC C LIMIT 3;
 | 1
 | 2
 | 3

ksql> SELECT C.ROWKEY, C.ID FROM CUSTOMERS C LIMIT 3;
1 | 1
2 | 2
3 | 3
----


=== Join live stream of ratings to customer data

[source,sql]
----
ksql> SELECT R.RATING_ID, R.CHANNEL, R.MESSAGE, C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME FROM RATINGS R LEFT JOIN CUSTOMERS C ON R.USER_ID = C.ID WHERE C.FIRST_NAME IS NOT NULL;
241 | android | (expletive deleted) | Bram Acaster
245 | web | Exceeded all my expectations. Thank you ! | Marigold Veld
247 | android | airport refurb looks great, will fly outta here more! | Modestia Coltart
251 | iOS-test | why is it so difficult to keep the bathrooms clean ? | Bob Argabrite
252 | iOS | more peanuts please | Marv Dalrymple
254 | web | why is it so difficult to keep the bathrooms clean ? | Marigold Veld
255 | iOS-test | is this as good as it gets? really ? | Ruperto Matteotti
257 | web | is this as good as it gets? really ? | Marigold Veld
259 | iOS-test | your team here rocks! | Bob Argabrite
----

Persist this stream of data

[source,sql]
----
ksql> CREATE STREAM RATINGS_ENRICHED WITH (PARTITIONS=1) AS SELECT R.RATING_ID, R.CHANNEL, R.STARS, R.MESSAGE, C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME FROM RATINGS R LEFT JOIN CUSTOMERS C ON R.USER_ID = C.ID WHERE C.FIRST_NAME IS NOT NULL ;
----

The `WITH (PARTITIONS=1)` is only necessary if the Elasticsearch connector has already been defined, as it will create the topic before KSQL does, and using a single partition (not 4, as KSQL wants to by default).

=== Examine changing reference data

Customers is a KSQL _table_, which means that we have the latest value for a given key.

Check out the ratings for customer id 2 only:
[source,sql]
----
ksql> SELECT * FROM RATINGS_ENRICHED WHERE ID=2;
----

In mysql, make a change to ID 2
[source,sql]
----
mysql> UPDATE CUSTOMERS SET FIRST_NAME = 'Thomas', LAST_NAME ='Smith' WHERE ID=2;
----

Observe in the continuous KSQL query that the customer name has now changed.

== View in Elasticsearch and Kibana

Tested on Elasticsearch 6.2.3.

=== Set up Kibana

* From http://localhost:5601/app/kibana#/management/kibana/index create a `ratings_enriched` Index Pattern

* From http://localhost:5601/app/kibana#/management/kibana/objects import `kibana_objects.json`

For some reason the mapping doesn't get picked up correctly. `curl -Xget "http://localhost:9200/ratings_enriched/_mapping/"` should show each text field as a `keyword`. If it doesn't, and the connector is running, simply run `curl -Xdelete "http://localhost:9200/ratings_enriched"` to truncate what's there and assuming the dynamic mapping has been created it will then get picked up when the index is then re-created.

=== View and explore data

image:images/es01.png[Kibana]

#EOF

== Optional


=== Aggregations

Simple aggregation - count of ratings per person, per minute:

[source,sql]
----
ksql> SELECT FULL_NAME,COUNT(*) FROM RATINGS_ENRICHED WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
----

Persist this and show the timestamp:

[source,sql]
----
CREATE TABLE RATINGS_PER_CUSTOMER_PER_MINUTE AS SELECT FULL_NAME,COUNT(*) AS RATINGS_COUNT FROM RATINGS_ENRICHED WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') , FULL_NAME, RATINGS_COUNT FROM RATINGS_PER_CUSTOMER_PER_MINUTE;
----
