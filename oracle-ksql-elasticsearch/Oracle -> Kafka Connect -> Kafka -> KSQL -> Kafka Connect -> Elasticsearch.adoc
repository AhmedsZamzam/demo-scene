= Oracle -> Kafka Connect -> Kafka -> KSQL -> Kafka Connect -> Elasticsearch

== Setup

=== Run Elasticsearch and Kibana

[source,bash]
----
elasticsearch
kibana
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.disk.threshold_enabled" : false
    }
}'
----

=== Set up Oracle

[source,bash]
----
git clone git@github.com:oracle/docker-images.git
# Follow instructions in `OracleDatabase/SingleInstance/README.md` to build the Docker image
docker run --name ora12c --expose 1521 --detach oracle/database:12.2.0.1-ee
docker logs -f ora12c
# wait until logs say "DATABASE IS READY TO USE!" then press ctrl-c to cancel the `docker logs`
docker exec ora12c ./setPassword.sh Password01
docker exec ora12c curl -s http://dominicgiles.com/swingbench/swingbench261082.zip -o swingbench.zip
docker exec ora12c unzip -q swingbench.zip
docker exec -ti ora12c bash
export JAVA_HOME=/opt/oracle/product/12.2.0.1/dbhome_1/jdk/jre/
export PATH=$PATH:$JAVA_HOME/bin:/home/oracle/swingbench/bin
oewizard -cs localhost:1521/ORCLPDB1 -df /opt/oracle/oradata/ORCLCDB/ORCLPDB1/soe.dbf -cl -scale 0.1 -dbap Password01 -create -u soe -p soe -ts soe -v

#docker exec ora12c export JAVA_HOME=/opt/oracle/product/12.2.0.1/dbhome_1/jdk/jre/
#docker exec ora12c export PATH=$PATH:$JAVA_HOME/bin:/home/oracle/swingbench/bin
#docker exec ora12c oewizard -cs localhost:1521/ORCLPDB1 -df /opt/oracle/oradata/ORCLCDB/ORCLPDB1/soe.dbf -cl -scale 0.1 -dbap Password01 -create -u soe -p soe -ts soe -v
----

=== Setup Elasticsearch

[source,bash]
----
brew install elasticsearch
brew install kibana
----

=== Setup Confluent Platform

( Install Confluent Platform 4.1 )

Copy `ojdbc8.jar` to `share/java/kafka-connect-jdbc/`

== Run

=== Run Confluent Platform, Elasticsearch, and Kibana

[source,bash]
----
elasticsearch
kibana
confluent start
----

=== Kafka Connect: Create connectors

[source,bash]
----
/Users/Robin/git/demo-scene/oracle-ksql-elasticsearch/create_connectors.sh
----

Check status:

[source,bash]
----
curl -s "http://localhost:8083/connectors"| jq '.[]'| xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| column -s : -t| sed 's/\"//g'| sort
es_sink_LOGON_ENRICHED            |  RUNNING  |  RUNNING
es_sink_ora-soe-ORDERS            |  RUNNING  |  RUNNING
jdbc_source_oracle_soe_customers  |  RUNNING  |  RUNNING
jdbc_source_oracle_soe_logon      |  RUNNING  |  RUNNING
jdbc_source_oracle_soe_orders     |  RUNNING  |  RUNNING
----

=== Run Swingbench

[source,bash]
----
docker exec -ti ora12c bash
export JAVA_HOME=/opt/oracle/product/12.2.0.1/dbhome_1/jdk/jre/
export PATH=$PATH:$JAVA_HOME/bin:/home/oracle/swingbench/bin
charbench -cs localhost:1521/ORCLPDB1 -u soe -p soe -v trans,users -c /home/oracle/swingbench/configs/SOE_Client_Side.xml -uc 1
----

=== KSQL : Inspect

[source,sql]
----
-- Only need this if not running on live data
--SET 'auto.offset.reset' = 'earliest';
CREATE STREAM LOGON WITH (KAFKA_TOPIC='ora-soe-LOGON', VALUE_FORMAT='AVRO', TIMESTAMP='LOGON_DATE');
SELECT * FROM LOGON LIMIT 5;
----


[source,sql]
----
CREATE STREAM CUST_SRC WITH (KAFKA_TOPIC='ora-soe-CUSTOMERS', VALUE_FORMAT='AVRO');
--SELECT CUSTOMER_ID, CUST_FIRST_NAME, CUST_LAST_NAME FROM CUST_SRC LIMIT 5;
CREATE STREAM CUST_REKEYED AS SELECT * FROM CUST_SRC PARTITION BY CUSTOMER_ID;
-- Wait a moment for the stream to be created
CREATE TABLE CUSTOMERS WITH (KAFKA_TOPIC='CUST_REKEYED', VALUE_FORMAT='AVRO', KEY='CUSTOMER_ID');
SELECT CUSTOMER_ID, CUST_FIRST_NAME, CUST_LAST_NAME, CUSTOMER_CLASS  FROM CUSTOMERS WHERE CUSTOMER_ID=42;
----

=== KSQL : Join

[source,sql]
----
SELECT L.LOGON_ID, C.CUSTOMER_ID, C.CUST_FIRST_NAME + ' ' + C.CUST_LAST_NAME AS CUST_FULL_NAME, \
       C.CUSTOMER_SINCE, C.CUSTOMER_CLASS, C.CREDIT_LIMIT \
FROM LOGON L \
     LEFT OUTER JOIN CUSTOMERS C \
     ON L.CUSTOMER_ID = C.CUSTOMER_ID \
WHERE C.CUSTOMER_ID IS NOT NULL;
----

[source,sql]
----
CREATE STREAM LOGON_ENRICHED AS \
SELECT L.LOGON_ID, C.CUSTOMER_ID, C.CUST_FIRST_NAME + ' ' + C.CUST_LAST_NAME AS CUST_FULL_NAME, \
       C.CUSTOMER_SINCE, C.CUSTOMER_CLASS, C.CREDIT_LIMIT \
FROM LOGON L \
     LEFT OUTER JOIN CUSTOMERS C \
     ON L.CUSTOMER_ID = C.CUSTOMER_ID \
WHERE C.CUSTOMER_ID IS NOT NULL;

SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss'), LOGON_ID, CUST_FULL_NAME, TIMESTAMPTOSTRING(CUSTOMER_SINCE,'yyyy-MM-dd HH:mm:ss'), CUSTOMER_CLASS, CREDIT_LIMIT FROM LOGON_ENRICHED;
----

Inspect output topic:

[source,bash]
----
kafka-avro-console-consumer \
   --bootstrap-server localhost:9092 \
   --property schema.registry.url=http://localhost:8081 \
   --topic LOGON_ENRICHED --max-messages 1 --from-beginning | jq '.'
----

=== KSQL : Filter

[source,sql]
----
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss'), LOGON_ID, CUST_FULL_NAME, \
       TIMESTAMPTOSTRING(CUSTOMER_SINCE,'yyyy-MM-dd HH:mm:ss'), CUSTOMER_CLASS, CREDIT_LIMIT \
FROM  LOGON_ENRICHED \
WHERE CREDIT_LIMIT > 4000;
----

[source,sql]
----
CREATE STREAM PRIME_LOGON AS \
SELECT * FROM LOGON_ENRICHED \
WHERE CUSTOMER_CLASS='Prime';

SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss'), LOGON_ID, CUST_FULL_NAME, TIMESTAMPTOSTRING(CUSTOMER_SINCE,'yyyy-MM-dd HH:mm:ss'), CUSTOMER_CLASS FROM PRIME_LOGON;
----

=== KSQL : Aggregate

[source,sql]
----
CREATE STREAM ORDERS WITH (KAFKA_TOPIC='ora-soe-ORDERS', VALUE_FORMAT='AVRO', TIMESTAMP='ORDER_DATE');

CREATE TABLE ORDERS_AGG_HOURLY AS \
SELECT ORDER_STATUS, COUNT(*) AS ORDER_COUNT, MAX(ORDER_TOTAL) AS MAX_ORDER_TOTAL, \
MIN(ORDER_TOTAL) AS MIN_ORDER_TOTAL, SUM(ORDER_TOTAL) AS SUM_ORDER_TOTAL, \
SUM(ORDER_TOTAL)/COUNT(*) AS AVG_ORDER_TOTAL \
FROM ORDERS WINDOW TUMBLING (SIZE 1 HOUR) \
GROUP BY ORDER_STATUS;

SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') , ORDER_COUNT, AVG_ORDER_TOTAL  \
FROM ORDERS_AGG_HOURLY \
WHERE ORDER_STATUS=2;
----

Reserve idea:
[source,sql]
----
SELECT CUSTOMER_CLASS, MAX(CREDIT_LIMIT) MAX_CREDIT_LIMIT, SUM(CREDIT_LIMIT)/COUNT(CREDIT_LIMIT) AS AVG_CREDIT_LIMIT \
FROM LOGON_ENRICHED WINDOW TUMBLING (SIZE 1 MINUTE) \
GROUP BY CUSTOMER_CLASS;
----

=== Analyse data in Kibana

_Import `kibana.json` into Kibana for pre-built viz & dashboard_
