= Streaming ETL demo - Enriching event stream data with CDC data from Oracle, stream into Elasticsearch
Robin Moffatt <robin@confluent.io>
v1.30, October 30, 2018

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

This is designed to be run as a step-by-step demo. The `ksql-statements.sql` should match those run in this doc end-to-end and in theory you can just run the file, but I have not tested it. PRs welcome for a one-click script that just demonstrates the end-to-end running demo :)

The slides that accompany this demo can be found here: https://speakerdeck.com/rmoff/apache-kafka-and-ksql-in-action-lets-build-a-streaming-data-pipeline

== Pre-reqs

Local:

* `curl`
* `jq`
* Docker

== Pre-Flight Setup

Start the environment

[source,bash]
----
cd docker-compose
./scripts/setup.sh
----

=== Run KSQL CLI and SQL*Plus

Optionally, use something like `screen` or `tmux` to have these both easily to hand. Or multiple Terminal tabs. Whatever works for you :)

* KSQL CLI:
+
[source,bash]
----
cd docker-compose
docker-compose exec ksql-cli ksql http://ksql-server:8088
----

* SQL*Plus
+
[source,bash]
----
cd docker-compose
docker-compose exec oracle bash -c 'sqlplus Debezium/dbz@localhost:1521/ORCLPDB1'
----


== Pre-flight checklist

* Load http://localhost:5601/app/kibana#/dashboard/Oracle-ksql-kafka-es?_g=(refreshInterval:('$$hashKey':'object:229',display:'30%20seconds',pause:!f,section:1,value:30000),time:(from:now-15m,mode:quick,to:now))&_a=(description:'',filters:!(),fullScreenMode:!f,options:(darkTheme:!f,hidePanelTitles:!f,useMargins:!t),panels:!((gridData:(h:15,i:'1',w:24,x:0,y:10),id:'0c118530-31d5-11e8-a6be-09f3e3eb4b97',panelIndex:'1',type:visualization,version:'6.3.0'),(gridData:(h:10,i:'2',w:48,x:0,y:35),id:'39803a20-31d5-11e8-a6be-09f3e3eb4b97',panelIndex:'2',type:visualization,version:'6.3.0'),(gridData:(h:10,i:'4',w:8,x:0,y:0),id:'5ef922e0-6ff0-11e8-8fa0-279444e59a8f',panelIndex:'4',type:visualization,version:'6.3.0'),(gridData:(h:10,i:'5',w:40,x:8,y:0),id:'2f3d2290-6ff0-11e8-8fa0-279444e59a8f',panelIndex:'5',type:search,version:'6.3.0'),(gridData:(h:15,i:'6',w:24,x:24,y:10),id:c6344a70-6ff0-11e8-8fa0-279444e59a8f,panelIndex:'6',type:visualization,version:'6.3.0'),(embeddableConfig:(),gridData:(h:10,i:'7',w:48,x:0,y:25),id:'11a6f6b0-31d5-11e8-a6be-09f3e3eb4b97',panelIndex:'7',sort:!(EXTRACT_TS,desc),type:search,version:'6.3.0')),query:(language:lucene,query:''),timeRestore:!f,title:'Ratings%20Data',viewMode:view)[Kibana ratings dashboard]
* Create iTerm window, using the `screencapture` profile
* Load this instructions doc into Chrome
* Close all other apps
* Optional: 
** Make sure phone is on wifi/internet
** `cd ios_push_notifications`
** `python push_bullet.py`
** Connect laptop to wifi
** Connect iPhone to laptop with USB
** Launch QuickTime Player, select iPhone as source
** Disable other notifications (WhatsApp/Messages/Slack etc)

== Demo

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

== Part 01 - Kafka Connect


=== Show Oracle table + contents

[source,sql]
----
COL FIRST_NAME FOR A15
COL LAST_NAME FOR A15
COL ID FOR 999
COL CLUB_STATUS FOR A12
COL EMAIL FOR A30
SET LINESIZE 200
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS 
FETCH FIRST 5 ROWS ONLY;

 ID FIRST_NAME      LAST_NAME       EMAIL                          CLUB_STATUS
--- --------------- --------------- ------------------------------ ------------
  1 Rica            Blaisdell       rblaisdell0@rambler.ru         bronze
  2 Ruthie          Brockherst      rbrockherst1@ow.ly             platinum
  3 Mariejeanne     Cocci           mcocci2@techcrunch.com         bronze
  4 Hashim          Rumke           hrumke3@sohu.com               platinum
  5 Hansiain        Coda            hcoda4@senate.gov              platinum

----

=== Check status of Debezium connectors

[source,bash]
----
curl -s "http://localhost:8083/connectors"| jq '.[]'| xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| column -s : -t| sed 's/\"//g'| sort
ora-source-demo-customers  |  RUNNING  |  RUNNING
----

=== Show Kafka topic has been created & populated

In KSQL: 

[source,sql]
----
ksql> list topics;

Kafka Topic                   | Registered | Partitions | Partition Replicas | Consumers | ConsumerGroups
-----------------------------------------------------------------------------------------------------------
_confluent-metrics            | false      | 12         | 1                  | 0         | 0
_schemas                      | false      | 1          | 1                  | 0         | 0
asgard.DEBEZIUM.CUSTOMERS     | true       | 1          | 1                  | 1         | 1
connect-status                | false      | 5          | 1                  | 0         | 0
my_connect_configs            | false      | 1          | 1                  | 0         | 0
my_connect_offsets            | false      | 25         | 1                  | 0         | 0
schema-changes.inventory      | false      | 1          | 1                  | 0         | 0
-----------------------------------------------------------------------------------------------------------
----


Show topic contents

[source,sql]
----
ksql> PRINT 'asgard.DEBEZIUM.CUSTOMERS' FROM BEGINNING;
Format:AVRO
11/30/18 10:44:27 AM UTC, , {"before": null, "after": {"ID": {"bytes": "\u0001"}, "FIRST_NAME": "Rica", "LAST_NAME": "Blaisdell", "EMAIL": "rblaisdell0@rambler.ru", "GENDER": "Female", "CLUB_STATUS": "bronze", "COMMENTS": "Universal optimal hierarchy", "CREATE_TS": 1543515952219218, "UPDATE_TS": 1543515952219218}, "source": {"version": "0.9.0.Alpha2", "connector": "oracle", "name": "asgard", "ts_ms": 1543574662454, "txId": null, "scn": 1755382, "snapshot": true}, "op": "r", "ts_ms": 1543574662472}
[...]
----


From the CLI: 

[source,bash]
----
docker-compose exec kafka-connect-cp \
                    kafka-avro-console-consumer \
                    --bootstrap-server kafka:29092 \
                    --property schema.registry.url=http://schema-registry:8081 \
                    --topic asgard.DEBEZIUM.CUSTOMERS --from-beginning
----

==== Insert a row in Oracle, observe it in Kafka

[source,sql]
----
INSERT INTO CUSTOMERS (FIRST_NAME,LAST_NAME,CLUB_STATUS) VALUES ('Rick','Astley','Bronze');
COMMIT;
----

==== Update a row in Oracle, observe it in Kafka

[source,sql]
----
UPDATE CUSTOMERS SET CLUB_STATUS = 'Platinum' where ID=42;
COMMIT;
----

---

Return to slides 

---

== Part 02 - KSQL for filtering streams

=== Inspect topics

[source,sql]
----
SHOW TOPICS;
----

=== Inspect ratings & define stream

[source,sql]
----
CREATE STREAM RATINGS WITH (KAFKA_TOPIC='ratings',VALUE_FORMAT='AVRO');
----

=== Filter live stream of data

[source,sql]
----
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;
----

=== Create a derived stream

[source,sql]
----
CREATE STREAM POOR_RATINGS AS \
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;

SELECT * FROM POOR_RATINGS LIMIT 5;

DESCRIBE EXTENDED POOR_RATINGS;
----

---

Return to slides 

---

== Part 03 - KSQL for joining streams

=== Inspect CUSTOMERS data
[source,sql]
----
-- Inspect raw topic data if you want
-- PRINT 'asgard.demo.CUSTOMERS' FROM BEGINNING;

SET 'auto.offset.reset' = 'earliest';
CREATE STREAM CUSTOMERS_STREAM_SRC WITH (KAFKA_TOPIC='asgard.DEBEZIUM.CUSTOMERS', VALUE_FORMAT='AVRO');

CREATE STREAM CUSTOMERS_STREAM WITH (PARTITIONS=1) AS SELECT * FROM CUSTOMERS_STREAM_SRC PARTITION BY ID;
SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_STREAM WHERE ID=1;
----

=== Re-key the customer data
[source,sql]
----
-- Wait for a moment here; if you run the CTAS _immediately_ after the CSAS it may fail
-- with error `Could not fetch the AVRO schema from schema registry. Subject not found.; error code: 40401`
-- You may also get this error if you have not set 'auto.offset.reset'='earliest' and there is no 
-- data flowing into the source CUSTOMERS topic, since no messages will have triggered the target stream 
-- to be created.
-- See https://github.com/confluentinc/ksql/issues/713
CREATE TABLE CUSTOMERS WITH (KAFKA_TOPIC='CUSTOMERS_STREAM', VALUE_FORMAT ='AVRO', KEY='ID');
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS WHERE ID=1;
----

==== [Optional] Demonstrate Stream / Table difference

Here's the stream - every event, which in this context is every change event on the source database: 

[source,sql]
----
ksql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS_STREAM WHERE ID=1;
1 | Rica | Blaisdell | rblaisdell0@rambler.ru | bronze
1 | Bob | Blaisdell | rblaisdell0@rambler.ru | bronze
1 | Fred | Blaisdell | rblaisdell0@rambler.ru | bronze
^CQuery terminated
ksql>
----

Here's the table - the latest value for a given key
[source,sql]
----
ksql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS WHERE ID=1;
1 | Fred | Blaisdell | rblaisdell0@rambler.ru | bronze
^CQuery terminated
----

=== Join live stream of ratings to customer data

[source,sql]
----
ksql> SELECT R.RATING_ID, R.MESSAGE, \
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, \
      C.CLUB_STATUS \
      FROM RATINGS R \
        LEFT JOIN CUSTOMERS C \
        ON R.USER_ID = C.ID \
      WHERE C.FIRST_NAME IS NOT NULL;
524 | Surprisingly good, maybe you are getting your mojo back at long last! | Patti Rosten | silver
525 | meh | Fred Blaisdell | bronze
526 | more peanuts please | Hashim Rumke | platinum
527 | more peanuts please | Laney Toopin | platinum
529 | Exceeded all my expectations. Thank you ! | Ruthie Brockherst | platinum
530 | (expletive deleted) | Brianna Paradise | bronze
â€¦
----

Persist this stream of data

[source,sql]
----
CREATE STREAM RATINGS_WITH_CUSTOMER_DATA \
       WITH (PARTITIONS=1, \
             KAFKA_TOPIC='ratings-enriched') \
       AS \
SELECT R.RATING_ID, R.MESSAGE, R.STARS, R.CHANNEL,\
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, \
      C.CLUB_STATUS, C.EMAIL \
      FROM RATINGS R \
        LEFT JOIN CUSTOMERS C \
        ON R.USER_ID = C.ID \
      WHERE C.FIRST_NAME IS NOT NULL;
----

The `WITH (PARTITIONS=1)` is only necessary if the Elasticsearch connector has already been defined, as it will create the topic before KSQL does, and using a single partition (not 4, as KSQL wants to by default).

=== Examine changing reference data

CUSTOMERS is a KSQL _table_, which means that we have the latest value for a given key.

Check out the ratings for customer id 2 only:
[source,sql]
----
ksql> SELECT * FROM RATINGS_WITH_CUSTOMER_DATA WHERE ID=2;
----

In Oracle, make a change to ID 2
[source,sql]
----
Oracle> UPDATE CUSTOMERS SET FIRST_NAME = 'Thomas', LAST_NAME ='Smith' WHERE ID=2;
----

Observe in the continuous KSQL query that the customer name has now changed.

=== Create stream of unhappy VIPs

[source,sql]
----
CREATE STREAM UNHAPPY_PLATINUM_CUSTOMERS \
       WITH (VALUE_FORMAT='JSON', PARTITIONS=1) AS \
SELECT FULL_NAME, CLUB_STATUS, EMAIL, STARS, MESSAGE \
FROM   RATINGS_WITH_CUSTOMER_DATA \
WHERE  STARS < 3 \
  AND  CLUB_STATUS = 'platinum';
----

== View in Elasticsearch and Kibana

Tested on Elasticsearch 6.3.0

image:images/es01.png[Kibana]

---

Return to slides 

---

#EOF

== Optional


=== Aggregations

Simple aggregation - count of ratings per person, per minute:

[source,sql]
----
ksql> SELECT FULL_NAME,COUNT(*) FROM RATINGS_WITH_CUSTOMER_DATA WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
----

Persist this and show the timestamp:

[source,sql]
----
CREATE TABLE RATINGS_PER_CUSTOMER_PER_MINUTE AS SELECT FULL_NAME,COUNT(*) AS RATINGS_COUNT FROM ratings_with_customer_data WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') , FULL_NAME, RATINGS_COUNT FROM RATINGS_PER_CUSTOMER_PER_MINUTE;
----

=== Slack/PushBullet notifications

_This bit will need some config of your own, as you'll need your own Slack workspace and API key (both free). With this though, you can demo the idea of an event-driven app subscribing to a KSQL-populated stream of filtered events._

_A newer version of the push notification script uses PushBullet, see `ios_push_notifications/push_bullet.py`._

image:images/slack_ratings.png[Slack push notifications driven from Kafka and KSQL]

To run, first export your API key as an environment variable:

[source,bash]
----
export SLACK_API_TOKEN=xyxyxyxyxyxyxyxyxyxyxyx
----

Or if you've got it locally, run `source slack_creds.sh`

then run the code:

[source,bash]
----
python python_kafka_notify.py
----

You will need to install `slackclient` and `confluent_kafka` libraries.

=== XStream notes

Create outbound server in CDB, from there can specify data from CDB+PDB or just PDB. By default (`source_database`,`source_container_name` both NULL) will stream data from both. 
ref: https://docs.oracle.com/database/121/XSTRM/xstrm_gen_cncpt.htm#GUID-60589C03-2095-42AA-8F2F-AFD83B110F31__GUID-F2B0932E-2D66-4652-A805-A570A10E53C4


https://debezium.io/docs/connectors/oracle/

https://github.com/debezium/oracle-vagrant-box
https://github.com/debezium/debezium-examples/tree/master/tutorial#using-oracle

https://gitter.im/debezium/user

https://docs.oracle.com/database/121/ARPLS/d_xstrm_adm.htm
https://docs.oracle.com/database/121/XSTRM/xstrm_xout_mon.htm
https://docs.oracle.com/database/121/XSTRM/xstrm_xout_trouble.htm
https://docs.oracle.com/database/121/XSTRM/xstrm_xout_config.htm

https://groups.google.com/d/msg/debezium/Ujj9fj3aoGM/DihBfAR3AQAJ

