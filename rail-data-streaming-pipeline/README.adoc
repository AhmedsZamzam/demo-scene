= Building a Streaming Data Pipeline with Rail Data
Robin Moffatt <robin@confluent.io>
v1.20, 7 August 2019

== What is it?

A PoC showing what can be done with streaming and batch sources of data, Apache Kafka and KSQL, and various data stores and tools. 

image::images/overview.png[]

== What can it do? 

image::images/es_dashboard_01.png[]
image::images/telegram01.png[]
image::images/graph01.jpg[]
image::images/pgadmin.jpg[]

== Slides? 

Yes!

http://rmoff.dev/kafka-trains-slides

image::images/slides.png[]

== Setup

* Register for an account at https://datafeeds.networkrail.co.uk/
* Set username and password in 
+
[source,bash]
----
/data/credentials.properties
/data/set_credentials_env.sh
----

== Detailed pipeline view

https://docs.google.com/drawings/d/1xL5E1Zfj6YZcjbSI9aexBIZO_8wNVMsYhis96dTiJE4/edit?usp=sharing[Source]

image::images/pipeline.png[]

== Data ingest

N.B. A lot of the code is complete, but not documented below. The canonical version is the code; the docs below may or may not be accurate and/or complete. The code is sensibly named and laid out though so should be easy to follow. 

=== Streams

1. Create Kafka Connect connector(s):
+
[source,bash]
----
./data/ingest/movements/00_ingest.sh
----
+
Check status: 
+
[source,bash]
----
curl -s "http://localhost:8083/connectors"| jq '.[]'| xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| column -s : -t| sed 's/\"//g'| sort
----
+
[source,bash]
----
source-activemq-networkrail-TRAIN_MVT_EA_TOC-01  |  RUNNING  |  RUNNING
source-activemq-networkrail-TRAIN_MVT_ED_TOC-01  |  RUNNING  |  RUNNING
----
+
Check data: 
+
[source,bash]
----
kafkacat -b localhost:9092 -t networkrail_TRAIN_MVT -C -c1

{"messageID":"ID:opendata-backend.rockshore.net-42259-1558600921537-11:1:1:40:2938","messageType":"text","timestamp":1558693449805,"deliveryMode":2,"correlationID":null,"replyTo":null,"destination":{"desti
nationType":"topic","name":"TRAIN_MVT_ED_TOC"},"redelivered":false,"type":null,"expiration":1558693749805,"priority":4,"properties":{},"bytes":null,"map":null,"text":"[{\"header\":{\"msg_type\":\"0003\",\"
source_dev_id\":\"\",\"user_id\":\"\",\"original_data_source\":\"SMART\",\"msg_queue_timestamp\":\"1558693442000\",\"source_system_id\":\"TRUST\"},\"body\":{\"event_type\":\"ARR…
----

2. Set pipeline running to split out payload batches into single messages
+
[source,bash]
----
./data/ingest/movements/01_explode.sh &
----
+
[source,bash]
----
Group tm_explode rebalanced (memberid rdkafka-c53a4270-e767-493a-b5de-2244b389e645): assigned: networkrail_TRAIN_MVT [0]
% Reached end of topic networkrail_TRAIN_MVT [0] at offset 189
{"header":{"msg_type":"0003","source_dev_id":"","user_id":"","original_data_source":"SMART","msg_queue_timestamp":"1558694355000","source_system_id":"TRUST"},"body":{"event_type":"DEPARTURE","gbtt_timestamp":"","original_loc_stanox":"","planned_timestamp":"1558697700000","timetable_variation":"4","original_loc_timestamp":"","current_train_id":"","delay_monitoring_point":"false","next_report_run_time":"4","reporting_stanox":"00000","actual_timestamp":"1558697940000","correction_ind":"false","event_source":"AUTOMATIC","
…
----
+
Check the data on the target topic
+
[source,bash]
----
kafkacat -b localhost:9092 -t networkrail_TRAIN_MVT_X -C -c1
{"header":{"msg_type":"0003","source_dev_id":"","user_id":"","original_data_source":"SMART","msg_queue_timestamp":"1558694355000","source_system_id":"TRUST"},"body":{"event_type":"DEPARTURE","gbtt_timestamp":"","original_loc_stanox":"","planned_timestamp":"1558697700000","timetable_variation":"4","original_loc_timestamp":"","current_train_id":"","delay_monitoring_point":"false","next_report_run_time":"4","reporting_stanox":"00000","actual_timestamp":"1558697940000","correction_ind":"false","event_source":"AUTOMATIC","train_file_address":null,"platform":"","division_code":"20","train_terminated":"false","train_id":"321B74MH24","offroute_ind":"false","variation_status":"LATE","train_service_code":"11817020","toc_id":"20","loc_stanox":"32540","auto_expected":"true","direction_ind":"UP","route":"1","planned_event_type":"DEPARTURE","next_report_stanox":"32557","line_ind":""}}
----


=== Batch

[source,bash]
----
source ./data/set_credentials_env.sh
./data/ingest/movements/01_load_canx_reason_code.sh
./data/ingest/cif_schedule/00_ingest_schedule.sh
./data/ingest/cif_schedule/01_process_schedule.sh
----

[source,bash]
----
$ kafkacat -b localhost:9092 -t CIF_FULL_DAILY -C -c1
{"JsonTimetableV1":{"classification":"public","timestamp":1558653193,"owner":"Network Rail","Sender":{"organisation":"Rockshore","application":"NTROD","component":"SCHEDULE"},"Metadata":{"type":"full","sequence":2535}}}

$ kafkacat -b localhost:9092 -t JsonScheduleV1 -C -c1
{"CIF_bank_holiday_running":null,"CIF_stp_indicator":"P","CIF_train_uid":"C02207","applicable_timetable":"Y","atoc_code":"NT","new_schedule_segment":{"traction_class":"","uic_code":""},"schedule_days_runs":"0000010","schedule_end_date":"2019-12-14","schedule_segment":{"signalling_id":"5A99","CIF_train_category":"EE","CIF_headcode":"5102","CIF_course_indicator":1,"CIF_train_service_code":"21791000","CIF_business_sector":"??","CIF_power_type":"DMU","CIF_timing_load":"A","CIF_speed":"075","CIF_operating_characteristics":null,"CIF_train_class":null,"CIF_sleepers":null,"CIF_reservations":null,"CIF_connection_indicator":null,"CIF_catering_code":null,"CIF_service_branding":"","schedule_location":[{"location_type":"LO","record_identity":"LO","tiploc_code":"DLTN","tiploc_instance":null,"departure":"2319","public_departure":null,"platform":"1","line":null,"engineering_allowance":null,"pathing_allowance":null,"performance_allowance":null},{"location_type":"LI","record_identity":"LI","tiploc_code":"FYHLSJN","tiploc_instance":null,"arrival":null,"departure":null,"pass":"2331","public_arrival":null,"public_departure":null,"platform":null,"line":null,"path":null,"engineering_allowance":null,"pathing_allowance":null,"performance_allowance":null},{"location_type":"LI","record_identity":"LI","tiploc_code":"TURSDLJ","tiploc_instance":null,"arrival":null,"departure":null,"pass":"2333","public_arrival":null,"public_departure":null,"platform":null,"line":null,"path":null,"engineering_allowance":null,"pathing_allowance":null,"performance_allowance":null},{"location_type":"LI","record_identity":"LI","tiploc_code":"DRHM","tiploc_instance":null,"arrival":null,"departure":null,"pass":"2339","public_arrival":null,"public_departure":null,"platform":"DM","line":null,"path":null,"engineering_allowance":"1","pathing_allowance":null,"performance_allowance":null},{"location_type":"LI","record_identity":"LI","tiploc_code":"BRTLYJN","tiploc_instance":null,"arrival":null,"departure":null,"pass":"2347","public_arrival":null,"public_departure":null,"platform":null,"line":null,"path":null,"engineering_allowance":null,"pathing_allowance":null,"performance_allowance":null},{"location_type":"LI","record_identity":"LI","tiploc_code":"KEBGSJN","tiploc_instance":null,"arrival":null,"departure":null,"pass":"2350H","public_arrival":null,"public_departure":null,"platform":null,"line":null,"path":null,"engineering_allowance":null,"pathing_allowance":null,"performance_allowance":null},{"location_type":"LT","record_identity":"LT","tiploc_code":"NWCSTLE","tiploc_instance":null,"arrival":"2353","public_arrival":null,"platform":"6","path":null}]},"schedule_start_date":"2019-05-25","train_status":"P","transaction_type":"Create","last_schedule_segment":{"location_type":"LT","record_identity":"LT","tiploc_code":"NWCSTLE","tiploc_instance":null,"arrival":"2353","public_arrival":null,"platform":"6","path":null}}
----

== KSQL

=== CIF (schedule / train attribute data & location data)

[source,sql]
----
run script '/data/ksql/02_cif_schedule/01_schedule_raw.ksql';
run script '/data/ksql/02_cif_schedule/02_tiploc.ksql';
run script '/data/ksql/02_cif_schedule/03_schedule.ksql';
run script '/data/ksql/02_cif_schedule/04_schedule_table.ksql';
----

If you get `Avro schema for message values on topic TIPLOC_FLAT_KEYED does not exist in the Schema Registry.` after running `02_tiploc.ksql` then wait a few moments and then re-run it; the topic on which it's built is created and populated by the preceeding step, and it may not have kicked in yet. 

[source,sql]
----
ksql> SHOW STREAMS;

 Stream Name       | Kafka Topic       | Format
------------------------------------------------
 STANOX_FLAT_KEYED | STANOX_FLAT_KEYED | AVRO
 JSONSCHEDULEV1    | JsonScheduleV1    | JSON
 TIPLOC_FLAT_KEYED | TIPLOC_FLAT_KEYED | AVRO
 SCHEDULE_RAW      | CIF_FULL_DAILY    | JSON
 SCHEDULE_02       | SCHEDULE_02       | AVRO
 SCHEDULE_00       | SCHEDULE_00       | AVRO
 SCHEDULE_01       | SCHEDULE_01       | AVRO
 […]
----


[source,sql]
----
SELECT SCHEDULE_KEY,
       TRAIN_STATUS,
       POWER_TYPE,
       SEATING_CLASSES,
       DEPARTURE_TPS_DESCRIPTION, DEPARTURE_PUBLIC_DEPARTURE_TIME,
       ARRIVAL_TPS_DESCRIPTION, ARRIVAL_PUBLIC_ARRIVAL_TIME 
  FROM SCHEDULE_02_T
 WHERE DEPARTURE_PUBLIC_DEPARTURE_TIME IS NOT NULL
 LIMIT 1;
----

[source,sql]
----
Y62982/2019-09-03/O | Passenger & Parcels (Permanent - WTT) | Electric Multiple Unit | Standard only | BRADFORD FORSTER SQUARE | 1841 | SKIPTON | 1922
Limit Reached
Query terminated
----

[source,sql]
----
ksql> show tables;

 Table Name    | Kafka Topic       | Format | Windowed
-------------------------------------------------------
 TIPLOC        | TIPLOC_FLAT_KEYED | AVRO   | false
 STANOX        | STANOX_FLAT_KEYED | AVRO   | false
 SCHEDULE_02_T | SCHEDULE_02       | AVRO   | false
-------------------------------------------------------
----

[source,sql]
----
SET 'auto.offset.reset' = 'earliest';

SELECT TIPLOC_CODE, 
       NALCO,
       STANOX, 
       CRS_CODE,
       DESCRIPTION,
       TPS_DESCRIPTION
  FROM TIPLOC
 WHERE DESCRIPTION='ILKLEY' 
 LIMIT 1;
----

[source,sql]
----
ILKLEY | 856800 | 17055 | ILK | ILKLEY | ILKLEY
Limit Reached
Query terminated
----

=== Train Movements

[source,sql]
----
RUN SCRIPT '/data/ksql/03_movements/01_canx_reason.ksql';
RUN SCRIPT '/data/ksql/03_movements/01_movement_raw.ksql';
RUN SCRIPT '/data/ksql/03_movements/02_activations.ksql';
RUN SCRIPT '/data/ksql/03_movements/02_cancellations.ksql';
RUN SCRIPT '/data/ksql/03_movements/02_movements.ksql';
RUN SCRIPT '/data/ksql/03_movements/03_activations_table.ksql';
RUN SCRIPT '/data/ksql/03_movements/04_cancellations_activations.ksql';
RUN SCRIPT '/data/ksql/03_movements/04_movements_activations.ksql';
RUN SCRIPT '/data/ksql/03_movements/05_cancellations_activations_schedules.ksql';
RUN SCRIPT '/data/ksql/03_movements/05_movements_activations_schedules.ksql';
----

==== Movements

[source,sql]
----
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') as ACTUAL_TIMESTAMP,
       EVENT_TYPE,
       DEPARTURE_TPS_DESCRIPTION,
       ARRIVAL_TPS_DESCRIPTION,
       PLATFORM,
       CASE WHEN VARIATION_STATUS = 'ON TIME' THEN 'ON TIME' 
            WHEN VARIATION_STATUS = 'LATE' THEN CAST(TIMETABLE_VARIATION AS VARCHAR) + ' MINS LATE' 
            WHEN VARIATION_STATUS='EARLY' THEN CAST(TIMETABLE_VARIATION AS VARCHAR) + ' MINS EARLY' 
         END AS VARIATION ,
       VARIATION_STATUS,
       TOC,
       TRAIN_ID, SCHEDULE_KEY
  FROM TRAIN_MOVEMENTS_ACTIVATIONS_SCHEDULE_00 ;
  WHERE DEPARTURE_TPS_DESCRIPTION = 'BEN RHYDDING';
----

[source,sql]
----
2019-05-24 11:42:00 | ARRIVAL | BEN RHYDDING | Platform 1 | 1 MINS LATE | LATE | Arriva Trains Northern | 172D48MI24 | 11821020
2019-05-24 11:42:00 | ARRIVAL | BEN RHYDDING | Platform 2 | ON TIME | ON TIME | Arriva Trains Northern | 172V27MJ24 | 11819020
2019-05-24 11:43:00 | DEPARTURE | BEN RHYDDING | Platform 2 | 1 MINS LATE | LATE | Arriva Trains Northern | 172V27MJ24 | 11819020
2019-05-24 11:43:00 | DEPARTURE | BEN RHYDDING | Platform 1 | 2 MINS LATE | LATE | Arriva Trains Northern | 172D48MI24 | 11821020
2019-05-24 11:54:00 | ARRIVAL | BEN RHYDDING | Platform 1 | ON TIME | ON TIME | Arriva Trains Northern | 172V26MJ24 | 11819020
----

==== Cancellations

TODO

== Topic config

Once all pipelines are up and running, execute `/data/configure_topics.sh` to set the retention period to 26 weeks on each topic. 



== TODO

* Automate ingest & monitoring
** currently cron, replace with Apache Airflow?
* Ad-hoc visual analysis
** Superset? Google Data Studio? AWS Quicksight?
* Finish this README
